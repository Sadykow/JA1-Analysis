%  Ensemble optimisation
% Explanation
%
\subsubsection{Ensemble Optimisation with Nesterov's Momentum Adam and AdaMax}
The Adam algorithm is the most commonly used optimiser.
% The reason behind changing it to another lie in two potential issues.
However, two potential issues motivate the change to a different algorithm.
First, the training may not converge~\cite{reddi_convergence_2019}, and second, the optimal solution is frequently missed at large learning steps~\cite{wilson_marginal_2017}.

Xiao et al.~\cite{xiao_accurate_2019} proposed a novel alternative, combining several optimisers to address these issues.
The new ensemble optimisation algorithm was based on the combination of Nesterov's momentum Adam (Nadam), \mbox{Algorithm~\ref{alg:nadam}}~\cite{dozat_nadam_2016} and the AdaMax, \mbox{Algorithm~\ref{alg:adamax}}~\cite{kingma_adam_2017}, at certain training points.

% Nadam's advantage over Adam
%
The Nadam optimiser \mbox{Algorithm~\ref{alg:nadam}} extends the Adam optimiser, which implements the Nesterov momentum~\cite{dozat_nadam_2016}.
\mbox{Algorithm~\ref{alg:nadam}, Lines~\ref{alg:Nadam-Line} and~\ref{alg:Nadam-Line-update}} add additional calculations involving gradient and parameter updates, which are intended to improve convergence~speed.
% \textcolor{red}{Another error. This time with SQRT. According to Adam and the source code in TF for Nadam, epsilon is not inside sqrt(). Although the original article stands otherwise, Xiao also corrected it. I am sticking with whatever I have at TF. There is no $\beta_i$ and no $\beta_2$ in the second moment update equation. All of that was corrected.}
\begin{algorithm}[H]%\captionsetup{labelfont={sc,bf}, labelsep=newline}
\caption{Nesterov's adaptive moment estimation (Nadam) optimisation.}
\begin{algorithmic}[1]
  \STATE \textbf{Number of input samples} \\ $N\gets length(\textit{input data})$\\
  \STATE \textbf{Size of windows} \\ $S\gets length(V_{i..n})$\\
  \STATE Input: $x_n = [V_{i..n}, I_{i..n}, T_{i..n}] - $Shape: $X = (N, S, 3)$
  \STATE Output:$y_n = [SoC_{n}] - $Shape:$Y = (N, 1)$
  \STATE Define Loss function: $L$ \\
  Get hyperparameters: $\alpha, \beta_1, \beta_2, \epsilon$
  \WHILE{$W_t \text{ does not converge}$}
    \STATE $t \gets t+1$
    \STATE $g_t \gets \nabla_W L_t (W_{t-1})$ \COMMENT{obtain gradient}
    \STATE $m_t \gets \beta_1 m_{t-1}+(1-\beta_1) g_t $ \COMMENT{first moment calculation}
    \STATE $\upsilon_t \gets \beta_2 \upsilon_{t-1}+ \left(1-\beta_2 \right)g^2_t $ \COMMENT{2nd moment calculation}
    \STATE $\hat{m_t} \gets \frac{m_t}{1-\beta^t_1}$ \COMMENT{corrected $\hat{m_t}$}
    \STATE $\hat{\upsilon_t} \gets \frac{\upsilon_t}{1-\beta^t_2} $ \COMMENT{corrected $\hat{\upsilon_t}$}
    \STATE $\hat{g_t} \gets \frac{g_t}{1-\prod\nolimits_{i = 1}^{k}\beta^t_2} $ \COMMENT{corrected $\hat{g_t}$\label{alg:Nadam-Line}}
    \STATE $W_t \gets W_{t-1}-\alpha
    \frac{\left(\beta^{k+1}_1\hat{m_t}+\left(1-\beta^t_1\right)\hat{g_t}\right)}
    {\sqrt{\hat{\upsilon_t}}+\epsilon}$
    \COMMENT{Update parameters\label{alg:Nadam-Line-update}}
  \ENDWHILE
  \end{algorithmic}
  \label{alg:nadam}
\end{algorithm}

% AdaMax advantages over Adam
%
\mbox{Algorithm~\ref{alg:adamax}} in the ensemble sequence is AdaMax~\cite{kingma_adam_2017}, another modification of the Adam.
The second-order moment on \mbox{Algorithm~\ref{alg:adamax}, Line~\ref{alg:AdaMax-Line}} is replaced with the infinity norm.
As a result, Xiao et al.~\cite{xiao_accurate_2019} considered AdaMax to have a stable weight-updating rule and be suitable for use in the fine-tuning phase, since its advantage lies in the reduction in gradient fluctuations.
\begin{algorithm}[H]%\captionsetup{labelfont={sc,bf}, labelsep=newline}
  \caption{Adaptive moment estimation based on the infinity norm (Adamax).}
  \begin{algorithmic}[1]
    \STATE \textbf{Number of input samples} \\ $N\gets length(\textit{input data})$\\
    \STATE \textbf{Size of windows} \\ $S\gets length(V_{i..n})$\\
    \STATE Input: $x_n = [V_{i..n}, I_{i..n}, T_{i..n}] - $Shape: $X = (N, S, 3)$
    \STATE Output:$y_n = [SoC_{n}] - $Shape:$Y = (N, 1)$
    \STATE Define Loss function: $L$ \\
    Get hyperparameters: $\alpha, \beta_1, \beta_2, \epsilon$
    \WHILE{$W_t \text{ does not converge}$}
      \STATE $t \gets t+1$
      \STATE $g_t \gets \nabla_W L_t (W_{t-1})$ \COMMENT{obtain gradient}
      \STATE $m_t \gets \beta_1 m_{t-1}+(1-\beta_1) g_t $ \COMMENT{1st moment calculation}
      \STATE $\upsilon_t \gets max\left(\beta_2\upsilon_{t-1}, |g_t|\right) $ \COMMENT{corrected $\hat{\upsilon_t}$ \label{alg:AdaMax-Line}}
      \STATE $W_t \gets W_{t-1}- \alpha \frac{m_t}{(1-\beta^t_1)(\upsilon_t+\epsilon)} $ \COMMENT{update parameters}
    \ENDWHILE
  \end{algorithmic}
  \label{alg:adamax}
\end{algorithm}

%
%
Xiao et al.~\cite{xiao_accurate_2019} considered separating the training process into two stages: pretraining and fine-tuning.
Based on their observations: ``The purpose of the pre-training phase is to endow the GRU model with the appropriate parameters to capture inherent features of the training samples.
The Nadam algorithm uses adaptive learning rates and approximates the gradient using the Nesterov momentum, thereby ensuring fast convergence of the pre-training process.''~\cite{xiao_accurate_2019}, p.~54195.
The selection of the second algorithm was trivial.
Xiao et al.'s~\cite{xiao_accurate_2019} selection of AdaMax was defined by its fast convergence to a more stable value for further parameter adjustment.
% \textcolor{red}{I'll be cursed. No, epsilon does. In the TensorFlow implementation, epsilons are added to the vt. Corrected formula added, but commented out. \\
%Remarks made by Xiao justify the usage of the Ensemble algorithm. How would I reprase it into something shorter}\\
%\textit{Remark 1:} The purpose of the pre-training phase os to endow the GRU model with the appropriate parameters to capture the inherent features of the training samples. The Nadam algorithm uses adaptive learning rates and approximates the gradient through the Nesterov momentum, ensuring fast convergence of the pre0training process.
%\textit{Remark 2:} The purpose of the fine-tuning phase is to adjust the parameters further to achieve greater accuracy using the AdaMax algorithm, which converges to a more stable value. \\ \\
The proposed ensemble algorithm combined both methods for a single GRU's training, see \mbox{Algorithm~\ref{alg:ENS}}.
This algorithm describes the adapted version of the ensemble algorithm, used by the model training procedures, with Nadam for pretraining and AdaMax for fine-tuning phases.
From the results of Xiao et al.'s~\cite{xiao_accurate_2019} work, $<p_{1}$ and $<p_{2}$ had the same number of epochs---100.
This scenario used the value of $<p_{2}$ at the moment the model reached an overfit with the first phase.
The learning rate value was set to the minimum possible amount, as defined by the research literature.

\begin{algorithm}[H]%\captionsetup{labelfont={sc,bf}, labelsep=newline}
  \caption{Ensemble optimisation training process.}
  \begin{algorithmic}[1]
    \STATE Setup model. Split total number of epoch by 30\% to $p_{1}$ and $p_{2}$ or until model overfits at $p_{2}$
    \STATE Initialise parameters
    \WHILE{epoch $<p_{1}$:}
      \IF{epoch $<p_{2}$:}
        \STATE \COMMENT{pass if already compiled with Nadam}
        \STATE compile model with Nadam parameters \COMMENT{pretraining phase}
      \ELSE
        \STATE \COMMENT{pass if already compiled with AdaMax}
        \STATE compile model with AdaMax parameters \COMMENT{fine-tuning phase}
      \ENDIF
    \STATE train for a single epoch
    \ENDWHILE
  \end{algorithmic}
  \label{alg:ENS}
\end{algorithm}
%\textcolor{red}{Descibe what shape has been used. Single sample, history length, batch size (if applicable), features number}

%
%
% \mbox{Table~\ref{tab:ensemble-params}} higlights hyper parameters selected by Xiao et al.~\cite{xiao_accurate_2019}.
% \begin{table}[htbp]
%     \renewcommand{\arraystretch}{1.3}
%     \caption{Ensemble, specific parameters}
%     \centering
%     \label{tab:ensemble-params}
%     \resizebox{\columnwidth}{!}{
%     \begin{tabular}{ l c c c c }
%         \hline\hline \\[-3mm]
%         Optimiser     & $\alpha$ & $\beta_1 $ & $\beta_2$ &   $\epsilon$ \\
%         \hline
%         Nadam
%                 & $0.001$ & $0.99$ & $0.999$ & $10^{-8}$ \\% 0.0000001
%         AdaMax
%                 & $5*10^{-4}$ & $0.99$ & $0.999$ & $10^{-8}$ \\% $10^{-8}$
%         \hline
%     \end{tabular}
%     }
% \end{table}
