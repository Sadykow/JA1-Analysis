\subsubsection{Adam and Robust Online Adam}
The most commonly and by default used in Time-series prediction is Adaptive Moment Estimation~\cite{kingma_adam_2017} (Adam) optimiser.
The \mbox{Algorithm~\ref{alg:Adam}} highlights the steps required to update model weights and bias as per the source.
\begin{algorithm}\captionsetup{labelfont={sc,bf}, labelsep=newline}
  \caption{Adaptive Moment Estimation (Adam) optimisation}
  \begin{algorithmic}[1]
    \STATE \textbf{Number of input samples} \\ $N\gets length(\textit{input data})$\\
    \STATE \textbf{Size of windows} \\ $S\gets length(V_{i..n})$\\
    \STATE Input: $x_n = [V_{i..n}, I_{i..n}, T_{i..n}] - $Shape: $X = (N, S, 3)$
    \STATE Output:$y_n = [SoC_{n}] - $Shape:$Y = (N, 1)$
    \STATE Define Loss function: $L$ \\
           Get hyperparameters: $\alpha, \beta_1, \beta_2, \epsilon$
    \WHILE{$W_t \text{ not converge}$}
    \STATE $t \gets t+1$
    \STATE $g_t \gets \nabla_W L_t (W_{t-1})$ \COMMENT{Obtain gradient}
    \STATE $m_t \gets \beta_1 m_{t-1}+(1-\beta_1) g_t $ \COMMENT{$1_{st}$ moment calculation}
    \STATE $\upsilon_t \gets \beta_2 \upsilon_{t-1}+ \left(1-\beta_2 \right)g^2_t $ \COMMENT{$2_{nd}$ moment calculation \label{alg:Adam-Line-2Moment}}
    \STATE $\hat{m_t} \gets \frac{m_t}{1-\beta^t_1}$ \COMMENT{Corrected $\hat{m_t}$}
    \STATE $\hat{\upsilon_t} \gets \frac{\upsilon_t}{1-\beta^t_2} $ \COMMENT{Corrected $\hat{\upsilon_t}$}
    \STATE $W_t \gets W_{t-1}- \alpha \frac{\hat{m_t}}{\sqrt{\hat{\upsilon_t}}+\epsilon} $ \COMMENT{Update parameters}
    \ENDWHILE
  \end{algorithmic}
  \label{alg:Adam}
\end{algorithm}
On top of two constants $\beta$ used for momentum calculation, the algorithm uses $\epsilon$, which is referred to as fuzz factor.

%
%
Javid et al.~\cite{javid_adaptive_2020} extended the default $Adam$ algorithm introducing a Robust Online version of Adam, Line~\ref{alg:Adam-Line-2Moment}.
Adding the direct influence of loss function to the gradient update adds an online calculation to regular $Adam$ correction, \mbox{Algorithm~\ref{alg:RoAdam}}.
\begin{algorithm}\captionsetup{labelfont={sc,bf}, labelsep=newline}
  \caption{Robust Online Adaptive Moment Estimation (RoAdam) optimisation}
  \begin{algorithmic}[1]
    \STATE \textbf{Number of input samples} \\ $N\gets length(\textit{input data})$\\
    \STATE \textbf{Size of windows} \\ $S\gets length(V_{i..n})$\\
    \STATE Input: $x_n = [V_{i..n}, I_{i..n}, T_{i..n}] - $Shape: $X = (N, S, 3)$
    \STATE Output:$y_n = [SoC_{n}] - $Shape:$Y = (N, 1)$
    \STATE Define Loss function $L$ and initial loss $L_t = 1.0$ \\
           Get hyperparameters: $\alpha, \beta_1, \beta_2, \beta_3, \epsilon$
    \STATE Initialise: $m,v=zeroes$ and $d=ones$ \\
    \WHILE{$W_t \text{ not converge}$}
    \STATE $t \gets t+1$
    \STATE $g_t \gets \nabla_\phi L_t (W_{t-1})$ \COMMENT{Obtain gradient}
    \STATE $m_t \gets \beta_1 m_{t-1}+(1-\beta_1) g_t $ \COMMENT{$1_{st}$ moment calculation}
    \STATE $\upsilon_t \gets \beta_2 \upsilon_{t-1}+ \left(1-\beta_2 \right)g^2_t $ \COMMENT{$2_{nd}$ moment calculation}
    \STATE $\hat{m_t} \gets \frac{m_t}{1-\beta^t_1}$ \COMMENT{Corrected $\hat{m_t}$}
    \STATE $\hat{\upsilon_t} \gets \frac{\upsilon_t}{1-\beta^t_2} $ \COMMENT{Corrected $\hat{\upsilon_t}$}
    \STATE $r_t \gets \parallel L_t\left(W_{t-1}\right)/L_t\left(W_{t-2}\right) \parallel $ \COMMENT{Relative prediction error term of the loss function}
    \STATE $d_t \gets \beta_3 d_{t-1}+\left(1-\beta_3\right)r_t $ \COMMENT{$3^{rd}$ moment calculation}
    \STATE $W_t \gets W_{t-1}- \alpha \frac{\hat{m_t}}{d_t\sqrt{\hat{\upsilon_t}}+\epsilon} $ \COMMENT{Update parameters}
    \ENDWHILE
  \end{algorithmic}
  \label{alg:RoAdam}
\end{algorithm}
\mbox{Table~\ref{tab:adam-params}} highlights method-specific parameter selection for the optimiser.
\begin{table}[htbp]
  \renewcommand{\arraystretch}{1.3}
  \caption{Adam, specific parameters}
  \centering
  \label{tab:adam-params}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{ l l l l l l }
      \hline\hline \\[-3mm]
      Source     & $\alpha$ & $\beta_1 $ & $\beta_2$ & $\beta_3$ &  $\epsilon$ \\
      \hline
      \makecell[l]{Song et al.~\cite{song_lithium-ion_2018} \&\\Chemale et al.~\cite{Chemali2017}}
              & $0.001$ & $0.9$ & $0.999$ & |   &$10^{-8}$ \\% 0.0000001
      Javid et al.~\cite{javid_adaptive_2020}
              & $0.001$ & $0.9$ & $0.999$ & $0.999$ &$10^{-8}$ \\% 0.0000001
      \hline\hline
  \end{tabular}
  }
\end{table}

%
%
The framework library contained no inbuild implementation of the Robust optimiser.
Instead, it was implemented from the first principle and overwriting the model training procedure.
%
% IF
\ifthenelse {\boolean{thesis}}
%
% THEN
{
The code itself can be located in Appendix~\ref{app:RoAdam}.
}
{}
Unlike initial variables $m$, and $v$ of the Adam algorithm, which are set as a matrix of zeros, the $d$ variable in the RoAdam is set to start with ones.
In addition, the algorithm is dependant on the loss calculation and not every framework supports it.
The previous loss result has to be preserved into the next iteration.
The initial loss value must be set above zero as per the algorithm to avoid a zero-division error.