% RNN apliable to almost any time-series dependant problem
@article{sutskever2013training,
    added-at = {2018-05-22T00:12:48.000+0200},
    author = {Sutskever, Ilya},
    biburl = {https://www.bibsonomy.org/bibtex/20206a37e46da7a3c3e6a30cab839e6fb/dallmann},
    description = {scholar.googleusercontent.com/scholar.bib?q=info:LKn-6XMpQaAJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWwNGZ4Djj6gcJjdwauFg4drsvHEWsMfD&scisf=4&ct=citation&cd=-1&hl=en},
    interhash = {43074181bc20f9d8c1c5a97b29b73fb8},
    intrahash = {0206a37e46da7a3c3e6a30cab839e6fb},
    journal = {University of Toronto, Toronto, Ont., Canada},
    keywords = {deep_learning mlnlp phd rnn thesis},
    timestamp = {2018-05-22T00:12:48.000+0200},
    title = {Training recurrent neural networks},
    url = {http://www.cs.utoronto.ca/~ilya/pubs/ilya\_sutskever\_phd\_thesis.pdf},
    year = 2013
}
% Common activation functions
@misc{amidi_cs_2018,
	title = {{CS} 230 - {Recurrent} {Neural} {Networks} {Cheatsheet}},
	url = {https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks-architecture},
	urldate = {2021-01-24},
	author = {Amidi, Afshine and Amidi, Shervine},
	year = {2018},
	file = {CS 230 - Recurrent Neural Networks Cheatsheet:/mnt/WORK/Zotero/storage/KD7MYKGL/cheatsheet-recurrent-neural-networks.html:text/html}
}
% Not an actual reference to equation, but the best and closest I could find without referencing some stackexchange.
@misc{eckhardt_choosing_2018,
	title = {Choosing the right {Hyperparameters} for a simple {LSTM} using {Keras}},
	url = {https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046},
	abstract = {Choosing the right Hyperparameters for your model is often more an art than a science. Here is some intuition for good starting points.},
	language = {en},
	urldate = {2021-03-05},
	journal = {Medium},
	author = {Eckhardt, Karsten},
	month = nov,
	year = {2018},
	file = {Snapshot:/mnt/WORK/Zotero/storage/YSHTUSLU/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046.html:text/html}
}
% Vanishing gradient
@misc{rasifaghihi_predictive_2020,
	title = {Predictive {Analytics}: {LSTM}, {GRU} and {Bidirectional} {LSTM} in {TensorFlow}},
	shorttitle = {Predictive {Analytics}},
	url = {https://towardsdatascience.com/predictive-analysis-rnn-lstm-and-gru-to-predict-water-consumption-e6bb3c2b4b02},
	abstract = {A step-by-step tutorial on developing LSTM, GRU and Bi-Directional LSTM models to predict water consumption},
	language = {en},
	urldate = {2021-03-06},
	journal = {Medium},
	author = {Rasifaghihi, Niousha},
	month = aug,
	year = {2020},
	file = {Snapshot:/mnt/WORK/Zotero/storage/GL6QXYZW/predictive-analysis-rnn-lstm-and-gru-to-predict-water-consumption-e6bb3c2b4b02.html:text/html}
}
@article{hochreiter_vanishing_1998,
	title = {The {Vanishing} {Gradient} {Problem} {During} {Learning} {Recurrent} {Neural} {Nets} and {Problem} {Solutions}},
	volume = {06},
	issn = {0218-4885, 1793-6411},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218488598000094},
	doi = {10.1142/S0218488598000094},
	abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
	language = {en},
	number = {02},
	urldate = {2021-03-06},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Hochreiter, Sepp},
	month = apr,
	year = {1998},
	pages = {107--116},
	file = {Hochreiter - 1998 - The Vanishing Gradient Problem During Learning Rec.pdf:/mnt/WORK/Zotero/storage/SG3I2TTZ/Hochreiter - 1998 - The Vanishing Gradient Problem During Learning Rec.pdf:application/pdf}
}

% Battery Data Website
@misc{noauthor_calce_2017,
	title = {{CALCE} {Battery} {Research} {Group}},
	url = {https://web.calce.umd.edu/batteries/data.htm\#A123},
	urldate = {2021-03-15},
	journal = {Center for Advanced Life Cycle Engineering},
	year = {2017},
	file = {CALCE Battery Group:/mnt/WORK/Zotero/storage/2V8NQADH/data.html:text/html}
}

%
@misc{zhu_statefulnes_tfdocs_2020,
	type = {Documentation},
	title = {Recurrent {Neural} {Networks} ({RNN}) with {Keras}: {Cross}-batch statefulness{\textbar} {TensorFlow} {Core}},
	url = {https://www.tensorflow.org/guide/keras/rnn},
	language = {en},
	urldate = {2021-09-29},
	journal = {TensorFlow},
	author = {Zhu, Scott and Chollet, Fancois},
	month = apr,
	year = {2020},
	file = {Snapshot:/mnt/WORK/Zotero/storage/TP8FTYHI/rnn.html:text/html}
}
