% RNN apliable to almost any time-series dependant problem
@article{sutskever2013training,
    added-at = {2018-05-22T00:12:48.000+0200},
    author = {Sutskever, Ilya},
    biburl = {https://www.bibsonomy.org/bibtex/20206a37e46da7a3c3e6a30cab839e6fb/dallmann},
    description = {scholar.googleusercontent.com/scholar.bib?q=info:LKn-6XMpQaAJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAWwNGZ4Djj6gcJjdwauFg4drsvHEWsMfD&scisf=4&ct=citation&cd=-1&hl=en},
    interhash = {43074181bc20f9d8c1c5a97b29b73fb8},
    intrahash = {0206a37e46da7a3c3e6a30cab839e6fb},
    journal = {University of Toronto, Toronto, Ont., Canada},
    keywords = {deep_learning mlnlp phd rnn thesis},
    timestamp = {2018-05-22T00:12:48.000+0200},
    title = {Training recurrent neural networks},
    url = {http://www.cs.utoronto.ca/~ilya/pubs/ilya\_sutskever\_phd\_thesis.pdf},
    year = 2013
}
% Common activation functions
@misc{amidi_cs_2018,
	title = {{CS} 230 - {Recurrent} {Neural} {Networks} {Cheatsheet}},
	url = {https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks-architecture},
	urldate = {2021-01-24},
	author = {Amidi, Afshine and Amidi, Shervine},
	year = {2018},
	file = {CS 230 - Recurrent Neural Networks Cheatsheet:/mnt/WORK/Zotero/storage/KD7MYKGL/cheatsheet-recurrent-neural-networks.html:text/html}
}
% Not an actual reference to equation, but the best and closest I could find without referencing some stackexchange.
@misc{eckhardt_choosing_2018,
	title = {Choosing the right {Hyperparameters} for a simple {LSTM} using {Keras}},
	url = {https://towardsdatascience.com/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046},
	abstract = {Choosing the right Hyperparameters for your model is often more an art than a science. Here is some intuition for good starting points.},
	language = {en},
	urldate = {2021-03-05},
	journal = {Medium},
	author = {Eckhardt, Karsten},
	month = nov,
	year = {2018},
	file = {Snapshot:/mnt/WORK/Zotero/storage/YSHTUSLU/choosing-the-right-hyperparameters-for-a-simple-lstm-using-keras-f8e9ed76f046.html:text/html}
}
% Vanishing gradient
@misc{rasifaghihi_predictive_2020,
	title = {Predictive {Analytics}: {LSTM}, {GRU} and {Bidirectional} {LSTM} in {TensorFlow}},
	shorttitle = {Predictive {Analytics}},
	url = {https://towardsdatascience.com/predictive-analysis-rnn-lstm-and-gru-to-predict-water-consumption-e6bb3c2b4b02},
	abstract = {A step-by-step tutorial on developing LSTM, GRU and Bi-Directional LSTM models to predict water consumption},
	language = {en},
	urldate = {2021-03-06},
	journal = {Medium},
	author = {Rasifaghihi, Niousha},
	month = aug,
	year = {2020},
	file = {Snapshot:/mnt/WORK/Zotero/storage/GL6QXYZW/predictive-analysis-rnn-lstm-and-gru-to-predict-water-consumption-e6bb3c2b4b02.html:text/html}
}
@article{hochreiter_vanishing_1998,
	title = {The {Vanishing} {Gradient} {Problem} {During} {Learning} {Recurrent} {Neural} {Nets} and {Problem} {Solutions}},
	volume = {06},
	issn = {0218-4885, 1793-6411},
	url = {https://www.worldscientific.com/doi/abs/10.1142/S0218488598000094},
	doi = {10.1142/S0218488598000094},
	abstract = {Recurrent nets are in principle capable to store past inputs to produce the currently desired output. Because of this property recurrent nets are used in time series prediction and process control. Practical applications involve temporal dependencies spanning many time steps, e.g. between relevant inputs and desired outputs. In this case, however, gradient based learning methods take too much time. The extremely increased learning time arises because the error vanishes as it gets propagated back. In this article the de-caying error flow is theoretically analyzed. Then methods trying to overcome vanishing gradients are briefly discussed. Finally, experiments comparing conventional algorithms and alternative methods are presented. With advanced methods long time lag problems can be solved in reasonable time.},
	language = {en},
	number = {02},
	urldate = {2021-03-06},
	journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
	author = {Hochreiter, Sepp},
	month = apr,
	year = {1998},
	pages = {107--116},
	file = {Hochreiter - 1998 - The Vanishing Gradient Problem During Learning Rec.pdf:/mnt/WORK/Zotero/storage/SG3I2TTZ/Hochreiter - 1998 - The Vanishing Gradient Problem During Learning Rec.pdf:application/pdf}
}

% Battery Data Website
@misc{noauthor_calce_2017,
	title = {{CALCE} {Battery} {Research} {Group}},
	url = {https://web.calce.umd.edu/batteries/data.htm\#A123},
	urldate = {2021-03-15},
	journal = {Center for Advanced Life Cycle Engineering},
	year = {2017},
	file = {CALCE Battery Group:/mnt/WORK/Zotero/storage/2V8NQADH/data.html:text/html}
}

%
@misc{zhu_statefulnes_tfdocs_2020,
	type = {Documentation},
	title = {Recurrent {Neural} {Networks} ({RNN}) with {Keras}: {Cross}-batch statefulness{\textbar} {TensorFlow} {Core}},
	url = {https://www.tensorflow.org/guide/keras/rnn},
	language = {en},
	urldate = {2021-09-29},
	journal = {TensorFlow},
	author = {Zhu, Scott and Chollet, Fancois},
	month = apr,
	year = {2020},
	file = {Snapshot:/mnt/WORK/Zotero/storage/TP8FTYHI/rnn.html:text/html}
}

@article{xing_state_2014,
	title = {State of charge estimation of lithium-ion batteries using the open-circuit voltage at various ambient temperatures},
	volume = {113},
	issn = {0306-2619},
	url = {https://www.sciencedirect.com/science/article/pii/S0306261913005746},
	doi = {10.1016/j.apenergy.2013.07.008},
	abstract = {Ambient temperature is a significant factor that influences the accuracy of battery SOC estimation, which is critical for remaining driving range prediction of electric vehicles (EVs) and optimal charge/discharge control of batteries. A widely used method to estimate SOC is based on an online inference of open-circuit voltage (OCV). However, the fact that the OCV–SOC is dependent on ambient temperature can result in errors in battery SOC estimation. To address this problem, this paper presents an SOC estimation approach based on a temperature-based model incorporated with an OCV–SOC–temperature table. The unscented Kalman filtering (UKF) was applied to tune the model parameters at each sampling step to cope with various uncertainties arising from the operation environment, cell-to-cell variation, and modeling inaccuracy. Two dynamic tests, the dynamic stress test (DST) and the federal urban driving schedule (FUDS), were used to test batteries at different temperatures. Then, DST was used to identify the model parameters while FUDS was used to validate the performance of the SOC estimation. The estimation was made covering the major working range from 25\% to 85\% SOC. The results indicated that our method can provide accurate SOC estimation with smaller root mean squared errors than the method that does not take into account ambient temperature. Thus, our approach is effective and accurate when battery operates at different ambient temperatures. Since the developed method takes into account the temperature factor as well as the complexity of the model, it could be effectively applied in battery management systems for EVs.},
	language = {en},
	urldate = {2021-10-30},
	journal = {Applied Energy},
	author = {Xing, Yinjiao and He, Wei and Pecht, Michael and Tsui, Kwok Leung},
	month = jan,
	year = {2014},
	keywords = {Electric vehicles, Lithium-ion batteries, Open-circuit voltage, SOC estimation, Temperature-based model, Unscented Kalman filtering},
	pages = {106--115},
	file = {ScienceDirect Full Text PDF:/mnt/WORK/Zotero/storage/U6BRII8M/Xing et al. - 2014 - State of charge estimation of lithium-ion batterie.pdf:application/pdf;ScienceDirect Snapshot:/mnt/WORK/Zotero/storage/Q5LPCBHH/S0306261913005746.html:text/html},
}


@misc{noauthor_anr26650m1a,
	title = {{ANR26650M1A}},
	year = {2011},
	url = {https://www.master-instruments.com.au/products/61240/ANR26650M1A.html},
	abstract = {ANR26650M1A ANR26650M1A Lithium Iron Phosphate High Current Type Cylindrical Battery (Recommended maximum discharge current 70A) using patented Nanophosphate technology},
	urldate = {2021-10-30},
	file = {Snapshot:/mnt/WORK/Zotero/storage/5LYSVANX/ANR26650M1A.html:text/html},
}

@book{alma991010036879604001,
abstract = {"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Page 4 of cover.},
author = {Goodfellow, Ian},
address = {Cambridge, Massachusetts},
booktitle = {Deep learning},
isbn = {9780262035613},
keywords = {Machine learning},
language = {eng},
lccn = {2016022992},
publisher = {The MIT Press},
series = {Adaptive computation and machine learning},
title = {Deep learning },
year = {2016 - 2016},
}