\subsection{Model Training, Validation, and Testing Metrics Functions} \label{subsec:t_model}
%
% The evaluation process, how it is done and what it means to achieve
The evaluation procedures were conducted similarly to Mamo and \linebreak Wang's \cite{mamo_long_2020} research.
The same training, validation, and testing procedures were used to determine the best technique and performance.
%! Go more technical and complex
% Each training stage has been written from the first principle to focus on performance gain and implementation of published methods.
Each training stage was written from first principles using the CUDA-supported TensorFlow 2.3.1 framework's official documentation, with additional implementation to accommodate method modifications~\cite{chollet_writing_2020}.
This allowed a flexibility in modifying the training and evaluation procedures to come up with objective comparison criteria.
A single training iteration consisted of several stages, involving several performance and quality checks, to ensure that training improved until the optimum was achieved before hitting a limit.

% Save/load process to avoid overfitting early
%
Algorithm~\ref{alg:training} provides the pseudocode summary for the primary training procedures undertaken by every model.
Every training run worked with all three profiles, where the first cycle type was used for training and validation and another two for testing and performance rating, as illustrated in Figure~\ref{fig:cross-data}, with five charge/discharge cycles per type.
After a model setup, initial parameter definition, and a single-iteration (epoch) run with the entire training dataset, the models' mean average accuracy was compared with previous results to decide if retraining was necessary to foresee potential overfitting at Algorithm~\ref{alg:training} Line~\ref{alg:training-check}.
The interchange between different temperatures for validation was neglected to avoid unnecessary complexity in the comparison of the results.
If any improvement was observed, the model was saved as a checkpoint for a rollback if needed, before a follow-up evaluation, as in Algorithm~\ref{alg:training} Line~\ref{alg:training-succes}.
Otherwise, the model was rolled to the previous state and underwent another attempt, with a half-reduced current learning rate.
Within 3--8 attempts, the models recovered and continued learning with an error \mbox{between 3 and 6\%}, Algorithm~\ref{alg:training} Lines~\ref{alg:training-recovery-start} to~\ref{alg:training-recovery-end} as, after approximately 30--50 trials, they started to show obvious overfitting or underfitting results with no potential recovery.
The cycle-type train on Figure~\ref{fig:cross-data} demonstrates the one-to-five data breakdown between validation and training, where the cycle of 25 \textdegree{}C confirmed the general fitting process and produced an output for later use during performance averaging, as in Algorithm~\ref{alg:training} Line~\ref{alg:training-valid}.
For this investigation, a single-cycle verification at 25 \textdegree{}C was the primary criterion assessment, as per the research goal and limitations regarding the quantity of data that were provided.
Due to the potential assumption that a model inside an electric vehicle would undergo constant online learning, there was no situation when the model had to face completely unpredictable scenarios.
Finally, the model was tested against two other cycles types, as shown in Figure~\ref{fig:cross-data}, assessing the general state of charge cycling that was captured under different conditions, as in Algorithm~\ref{alg:training} Line~\ref{alg:training-test}.
Two cycles from 25 \textdegree{}C and 30 \textdegree{}C were selected as examples of the closest value ranges and most likely idle battery states.
The better the results, the more objective the model was in unseen conditions.
% An additional test against another battery takes place to verify the models' behaviour and the possibility for further improvement with new samples, which will not be part of performance measurements or discussion.
% The results from two other sample tests confirm that estimations are still rational within State of Charge behaviour boundaries.
If the model reached the optimum state, with no further accuracy improvements and hyperparameters to adjust, the training process was stopped and the model underwent the final evaluation against the entire training set with all three profiles, as in Algorithm~\ref{alg:training} Line~\ref{alg:training-end}.
%%%%%%%%%%%%%
\begin{algorithm}[H]%\captionsetup{labelfont={sc,bf}, labelsep=newline}
    \caption{Training procedure.}
    \begin{algorithmic}[1]
        \STATE Setup model. Define optimiser and metrics.
        \STATE Initialise parameters with initial learning rate at 0.001
        \STATE Set prev\_error 1
        \WHILE{epoch $<$ 100:}
            \STATE Train model, obtain gradients, and apply optimiser
            \IF{ error $>$ prev\_error:}  \label{alg:training-check}
                \WHILE{attempt $<$ 50:}
                    \STATE Load previous successful model \label{alg:training-recovery-start}
                    \STATE Reduce learning rate by half
                    \STATE Train model, obtain gradients, and apply optimiser
                    \IF{ error $<$ prev\_error:}
                        \STATE Update error. Save state. Break the loop. \label{alg:training-recovery-end}
                    \ENDIF
                \ENDWHILE
            \ELSE
                \STATE Update error. Save state. \label{alg:training-succes}
            \ENDIF
            \STATE Update learning rate based on the scheduler.
            \STATE Validate model on 25 \textdegree{}C cycler. \label{alg:training-valid}
            \STATE Test on two other profiles. \label{alg:training-test}
        \ENDWHILE
        \STATE Record overall results against entire training datasets. \label{alg:training-end}
    \end{algorithmic}
    \label{alg:training}
\end{algorithm}

% Accuracy evaluation explanation with drawings
%
% Testing performance gets measured against two cycles of each unsued scheduler using one of the available embedded devices capable of Machine Learning computation.
% The intention was to verify the model's capability to be used on low-power devices and still meet necessary performance requirements.
% The profile will be switched to another and perform the same training, validation and testing steps.
% This mechanism will output three models per method and nine figures outlining training and validation learning accuracies.
% Based on the progressive accuracy increase, a final iteration for this plot and discussion will be based on the lowest training and validation error.
The following process was adapted for all error calculations in training, verification, and testing.
Figure~\ref{fig:plot_demo} shows an example of the accuracy evaluation, where the actual state of charge is compared with the model's prediction.
The filled area below the plot captures the absolute error difference between two lines, as per Equation~(\ref{eq:abs-error}).
%%%%%%
The test procedure was performed on two cycles of each profile but at a different temperature to assess how perceptive the model was in capturing the average and high heat spikes that the battery module might experience.
Each result was summarised into a table and reported based on metrics values.
%%%%%%
\begin{equation}
    ABS error = \sqrt{(Actual-Prediction)^2}
    \label{eq:abs-error}
\end{equation}
%
%
% Finally, after completing all training, which results in 18 produced models, each will undertake an overall assessment.
% All outputs will be taken through the entire training set of three driving profiles from another cell, which was not part of the early training at all.
% It is expected that the lowest error will be against a dataset, which the model was trained on.
% That is why it was placed as a first record in the final comparison table.
% The report is based on the metrics, which were initially defined for training purposes.
% Following plot demonstrates how the multi-processing unit testing process is performed for all three driving profiles over a single model. 
%
% \subsubsection{Losses}~\label{subsub:losses}

%\mbox{Table~\ref{tab:experiment}} contains a separate column for every loss function, which has been applied to each tested model.
%Some equations were extracted directly from papers; others were written based on their definition and internal library implementation.
%The goal of the loss function is to calculate the error between model prediction and the actual value.
%The efficiencies and impact of each equation are hard to determine and are not the investigation's goal.
%The purpose of the table is only to provide the implementation references, which have been assumed during the programming of each published article-based model.
% \begin{table*}[htbp]
%     \renewcommand{\arraystretch}{1.3}
%     \caption{Model's loss functions}
%     \centering
%     \label{tab:losses}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{ l l l c }
%         \hline\hline \\[-3mm]
%         Function Name & Method used in & Source articles & Equation \\ 
%         \hline
%         Absolute Error & & Song \textit{et al.}~\cite{song_lithium-ion_2018} & $|SoC_t-\hat{SoC_t}|$ \\
%         \hline
%         -- &\#1 \#4 & \makecell[l]{Chemal \textit{et al.}\cite{Chemali2017},\\Jiao \textit{et al.}~\cite{jiao_gru-rnn_2020}} & $\sum\limits^N_{t=0} \frac{1}{2} (SoC_t-\hat{SoC_t})^2$ \\
%         \hline
%         \makecell[l]{Mean Average\\ Percentage Error} &\#3 & Mamo \textit{et al.}~\cite{mamo_long_2020} & $\frac{100}{N}\sum\limits^N_{t=0}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
%         \hline
%         -- &\#2 & Xiao \textit{et al.}~\cite{xiao_accurate_2019} & $\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}{N}$ \\
%         \hline
%         \makecell[l]{Mean Average\\ Squared Error} &\#5 \#6 & \makecell[l]{Javid \textit{et al.}~\cite{javid_adaptive_2020},\\Zhang \textit{et al.}~\cite{zhang_deep_2020}} & $\sum\limits^N_{t=0} \sqrt{\frac{1}{n} (SoC_t-\hat{SoC_t})^2}$ \\
%         \hline\hline
%     \end{tabular}
%     }
% \end{table*}

% $ L = \sum\limits^N_{t=0} \frac{1}{2} (SoC_t-\hat{SoC_t})^2$ ~\cite{Chemali2017},~\cite{jiao_gru-rnn_2020} \\
% $ L = \sum\limits^N_{t=0} \sqrt{\frac{1}{n} (SoC_t-\hat{SoC_t})^2}$ ~\cite{javid_adaptive_2020}~\cite{zhang_deep_2020} \\
% $ L = (\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2)N$~\cite{xiao_accurate_2019} \\
% $ L = |SoC_t-\hat{SoC_t}|$ \\
% $ L = \frac{100}{N}\sum\limits^N_{t=0}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$~\cite{mamo_long_2020} \\

% \ebd{table}
% $MAE = \frac{1}{N}\sum\limits^N_{t=1} |SoC_t-\hat{SoC_t}|$ \\
% $MAPE = \frac{100}{N}\sum\limits^N_{t=1}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
% $RMSE = \sqrt{\frac{1}{N}\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}$ \\
% $R^2 = 1-\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}
%               {\sum\limits^N_{t=1}(SoC_t-M_{SoC})^2}$ - $M_{SoC}$ is the mean SoC \\
% $Error = \frac{SoC_t-\hat{SoC_5}}{SoC_t}$ - Not decided yet
% \subsubsection{Callbacks}
% For the stateless model, the method for resetting a State of the model has been derived with the Callback function of:
% Another Algorithm. \\
% \begin{lstlisting}[language=Python]
% class ResetCallback(tf.keras.callbacks.Callback):
%     reset_steps : int = 500
%     i_counter   : int = 0
%     def __init__(self):
%         self.i_counter = 0

%     def on_batch_begin(self, batch, logs=None):
%         if (self.i_counter % self.reset_steps) == 0:
%             self.model.reset_states()
%             self.i_counter = 0
%         self.i_counter += 1
% \end{lstlisting}
% \begin{enumerate}
%     \item Checkpoint saving only the best one based on smalest rmse
%     \item Tensorboard tmp folder
%     \item Early stopping is applyable
%     \item Procedure of training epoch by epoch or algorithm for offline and online.
% \end{enumerate}
% \subsubsection{Callbacks}
% \begin{itemize}
%     \item Checkpoint \\
%     \item Tensorboard \\
%     \item nanTerminate \\    
% \end{itemize}
% \subsubsection{Training and validation Loop??}
% \begin{itemize}
%     \item validation over what data? - single cycle \\
% \end{itemize}
% \subsubsection{Testing procedures??}
% \begin{itemize}
%     \item Converting model to the TF-Lite usage for TPU processor \\
%     \item 2 profiles - two cycle test \\
% \end{itemize}

%
%
% \subsubsection{Metrics}
% \textcolor{red}{Generally, Losses start form 0, metrics from 1. Articles which did overwise were assumed to be mistaken.}
% \textcolor{red}{Similar to loses and reasons why we have chosen those as our criteria.} \\

% Metrics functions used through out the research
%
%! References on who used.
%! varied metric used across articles, we will used 3.
Metrics functions acted as user evaluation criteria to assess the performance of the trained model during both fitting and validation processes.
Although some papers relied on different evaluation criteria, for this research, the metrics were unified, with several equations provided in \mbox{Table~\ref{tab:metrics}}.
%! >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> Response 3 - Point 8
\added[id=Responce 3]{The mean average error (MAE) and root-mean-square error (RMSE) are the two standard metric functions used in almost any machine-learning-related problem, whereas the coefficient of determination (R2), a measure of a model's goodness of fit, has been used in several sources; therefore, to be as complete and comparable as possible, this was also used as one of the comparison criteria in this article.
All metrics were used to represent the entire training and testing cycles, which could be interpreted as a meaningful overall quality of the fitting process.}
%! <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Response 3 - Point 8
Thus, the same criteria were used to compare the model efficiencies.
\begin{figure}[ht]
    % \centering
    \includesvg[width=\textwidth]{II_Body/images/cross-data.svg}
    \caption{Three profiles cross data split for training, validation and testing in a simplistic SoC cycle representation through different temperatures}
    \label{fig:cross-data}
\end{figure}

\vspace{-18pt}

\begin{figure}[ht]
    % RMSE equation: RMS = (tf.keras.backend.sqrt(tf.keras.backend.square(y_test_one[::,]-PRED)))
    % \centering
    \includesvg[width=0.70\textwidth]{II_Body/images/plot-example.svg}
    \caption{Accuracy plot demonstration}
    \label{fig:plot_demo}
\end{figure}
\ifthenelse{\boolean{thesis}}{
\begin{table}[htbp]
    \renewcommand{\arraystretch}{1.3}
    \caption{Model's metrics functions}
    \centering
    \label{tab:metrics}
    \begin{tabular}{l c}
        \hline\hline \\[-3mm]
        Function Name & Equation \\ 
        \hline
        \makecell[l]{Mean Absolute\\ Error} &  $\frac{1}{N}\sum\limits^N_{t=1} |SoC_t-\hat{SoC_t}|$ \\
        % \hline
        % Mean Average Percentage Error & $\frac{100}{N}\sum\limits^N_{t=1}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
        \hline
        \makecell[l]{Root Mean\\ Square Error} & $ \sqrt{\frac{1}{N}\sum\limits^N_{t=1} \left(SoC_t-\hat{SoC_t} \right)^2}$ \\
        \hline
        \makecell[l]{$R^2$ : $M_{SoC}$ is\\ the mean SoC} & $1-\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}
                {\sum\limits^N_{t=1}(SoC_t-M_{SoC})^2}$ \\
        \hline\hline
        % Error : Not decided if useful yet & $ \frac{SoC_t-\hat{SoC_5}}{SoC_t}$
    \end{tabular}
\end{table}
}{
\begin{table}[H]
    \caption{Model's metrics functions.}
    \label{tab:metrics}
    \newcolumntype{C}{>{\centering\arraybackslash}X}
    \begin{tabularx}{\textwidth}{CC}
        \toprule
        \textbf{Function Name} & \textbf{Equation} \\
        \midrule
        Mean absolute error & $\frac{1}{N}\sum\limits^N_{t=1} |SoC_t-\hat{SoC_t}|$ \\
        \midrule
        Root-mean-square error & $ \sqrt{\frac{1}{N}\sum\limits^N_{t=1} \left(SoC_t-\hat{SoC_t} \right)^2}$ \\
        \midrule
        $R^2$ : $M_{SoC}$ is the mean SoC & $1-\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}
        {\sum\limits^N_{t=1}(SoC_t-M_{SoC})^2}$ \\
        \bottomrule
    \end{tabularx}
\end{table}
}

% Concluding purpose of the training and ultimate goad of getting so creative
%
% Overall, the primary criteria of the research are the ability to fit the training data, avoiding overfits, constant reasonably short retraining potential, susceptibility to undetermined conditions and final memory and processing usage.
% Ultimately, all models will be compared with their successes and faults to determine a path for potential improvement.
