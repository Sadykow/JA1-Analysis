\subsection{Model training, validation and testing metrics functions} \label{subsec:t_model}
% The evaluation process, how it is done and what it means to achieve
%
The methods evaluation procedures will be conducted similarly to Mamo \textit{et al.}~\cite{mamo_long_2020} research.
They will undertake the same training, validation and testing procedure to determine the best technique and performance.
%! Go more technical and complex
% Each training stage has been written from the first principle to focus on performance gain and implementation of published methods.
Each training stage has been written from the first principle using the CUDA-supported TensorFlow 2.3.1 framework's official documentation, with additional implementation to accommodate methods modifications\cite{chollet_writing_2020}.
It allows flexibility in modifying the training and evaluation procedures to come up with objective comparison criteria.
A single training iteration consists of several stages involving several performances and quality checks to ensure that training only keeps improving over time until the optimum is achieved before hitting a limit.

% Save/load process to avoid overfitting early
%
Algorith~\ref{alg:training} provides the pseudocode summary for the primary training procedures undertaken by every model.
Every training run works with all three profiles, where the first cycle type is used for training and validation, and another two for testing and performance rating, as illustrated in Figure~\ref{fig:cross-data} with five SoC cycles per type.
After a model setup, initial parameters definition and a single iteration (epoch) run with the entire training dataset, the models' mean average accuracy is compared with previous results to decide if retraining is necessary to foresight potential overfitting at Ag.~\ref{alg:training} ln.~\ref{alg:training-check}.
\textcolor{blue}{The cross interchanging between different temperatures for validation has been neglected to avoid unnecessary complexity for results comparison.}
If an improvement is observed, the model will be saved as a checkpoint for a rollback upon necessity, and the model continues to undertake a follow-up evaluation, Ag.~\ref{alg:training} ln.~\ref{alg:training-succes}.
Otherwise, the model will be rolled to the previous state and undertake another attempt with a half-reduced current learning rate.
Within 3 to 8 attempts, models recover and continue learning there as, by the time of reaching around 30-50, they start to show NaN results with no potential for recovery, with an error between 3-6\%, Ag.~\ref{alg:training} lns.~\ref{alg:training-recovery-start} to~\ref{alg:training-recovery-end}.
Cycle type train on Figure~\ref{fig:cross-data} demonstrates the one to five data breakdown between validation and training, where the cycle of 25\textdegree{}C confirms the general fitting process and produces an output for later use during performance averaging, Ag.~\ref{alg:training} ln.~\ref{alg:training-valid}.
In the end, it gets tested against two other cycles types on Figure~\ref{fig:cross-data}, assessing the general capture of the State of Charge cycling under different conditions, Ag.~\ref{alg:training} ln.~\ref{alg:training-test}.
Two cycles from each type of 25\textdegree{}C and 30\textdegree{}C were selected for illustrative purposes as the most value close ranges and most likely battery states at idle.
The better results, the more objective model becomes in unseen conditions.
% An additional test against another battery takes place to verify the models' behaviour and the possibility for further improvement with new samples, which will not be part of performance measurements or discussion.
For this investigation, verifying with a single cycle at 25\textdegree{}C was the primary criteria assessment, as per the research goal and limitation with provided data amount.
Due to the potential assumption that a model inside an electric vehicle will undertake constant online learning, there will be no situation when the model has to make completely unpredictable scenarios.
The results from two other sample tests confirm that estimations are still rational within State of Charge behaviour boundaries.
If the model reaches the optimum state, then there are no more accuracy improvements and hyper-parameter to adjust, the training process will be stopped and undertake the final evaluation against the entire training set with all three profiles, Ag.~\ref{alg:training} ln.~\ref{alg:training-end}.
\begin{algorithm}\captionsetup{labelfont={sc,bf}, labelsep=newline}
    \caption{Training procedure}
        \begin{algorithmic}[1]
            \STATE Setup model. Define optimiser and metrics.
            \STATE Initialise parameters with initial learning rate at 0.001
            \STATE Set prev\_error 1
            \WHILE{epoch  $<$100:}
                \STATE Train model, get gradients and apply optimiser
                \IF{ error $<$ prev\_error:}    \label{alg:training-check}
                    \WHILE{attempt  $<$ 50:}
                        \STATE Load previous successful model   \label{alg:training-recovery-start}
                        \STATE Reduce learning rate by half
                        \STATE Train model, get gradients and apply optimiser
                        \IF{ error $>$ prev\_error:}
                            \STATE Break the loop. Update error. Save state. \label{alg:training-recovery-end}
                        \ENDIF
                    \ENDWHILE
                \ELSE
                    \STATE Update error. Save state. \label{alg:training-succes}
                    \STATE Update learning rate based on the scheduler.
                \ENDIF
                \STATE Validate model on 25\textdegree{}C cycler.   \label{alg:training-valid}
                \STATE Test on two other profiles. \label{alg:training-test}
                % \STATE Test on a different cell.
            \ENDWHILE
            \STATE Record overall results against entire training datasets. \label{alg:training-end}
        \end{algorithmic}
    \label{alg:training}
\end{algorithm}
\begin{figure}[ht]
    \centering
    \includesvg[width=\columnwidth]{II_Body/images/cross-data.svg}
    \caption{Three profiles cross data split for training, validation and testing in a simplistic SoC cycle representation through different temperatures.}
    \label{fig:cross-data}
\end{figure}

% Accuracy evaluation explanation with drawings
%
% Testing performance gets measured against two cycles of each unsued scheduler using one of the available embedded devices capable of Machine Learning computation.
% The intention was to verify the model's capability to be used on low-power devices and still meet necessary performance requirements.
% The profile will be switched to another and perform the same training, validation and testing steps.
% This mechanism will output three models per method and nine figures outlining training and validation learning accuracies.
% Based on the progressive accuracy increase, a final iteration for this plot and discussion will be based on the lowest training and validation error.
\textcolor{red}{For all error calculations in training, verification and testing the following process was adapted.}
Figure~\ref{fig:plot_demo} shows an example of accuracy evaluation, where the actual State of Charge is compared with prediction.
The filled area below the plot captures the error Absolute Error Difference between two lines, as per Equation~\ref{eq:abs-error}.
%%%%%%
Test procedure performed on two cycles of each profile but at a different temperature to assess how perceptive the model is to capture average and high heat spikes at which accumulator may end up.
Each result is summarised into a table and reported based on metrics values.
%%%%%%
\begin{figure}[ht]
    % RMSE equation: RMS = (tf.keras.backend.sqrt(tf.keras.backend.square(y_test_one[::,]-PRED)))
    \centering
    \includesvg[width=\columnwidth]{II_Body/images/plot-example.svg}
    \caption{Accuracy plot demonstration.}
    \label{fig:plot_demo}
\end{figure}
\begin{equation}
    \textbf{ABS error}  = \sqrt{(Actual-Prediction)^2}
    \label{eq:abs-error}
\end{equation}
%
%
% Finally, after completing all training, which results in 18 produced models, each will undertake an overall assessment.
% All outputs will be taken through the entire training set of three driving profiles from another cell, which was not part of the early training at all.
% It is expected that the lowest error will be against a dataset, which the model was trained on.
% That is why it was placed as a first record in the final comparison table.
% The report is based on the metrics, which were initially defined for training purposes.
% Following plot demonstrates how the multi-processing unit testing process is performed for all three driving profiles over a single model. 
%
% \subsubsection{Losses}~\label{subsub:losses}

%\mbox{Table~\ref{tab:experiment}} contains a separate column for every loss function, which has been applied to each tested model.
%Some equations were extracted directly from papers; others were written based on their definition and internal library implementation.
%The goal of the loss function is to calculate the error between model prediction and the actual value.
%The efficiencies and impact of each equation are hard to determine and are not the investigation's goal.
%The purpose of the table is only to provide the implementation references, which have been assumed during the programming of each published article-based model.
% \begin{table*}[htbp]
%     \renewcommand{\arraystretch}{1.3}
%     \caption{Model's loss functions}
%     \centering
%     \label{tab:losses}
%     \resizebox{\linewidth}{!}{
%     \begin{tabular}{ l l l c }
%         \hline\hline \\[-3mm]
%         Function Name & Method used in & Source articles & Equation \\ 
%         \hline
%         Absolute Error & & Song \textit{et al.}~\cite{song_lithium-ion_2018} & $|SoC_t-\hat{SoC_t}|$ \\
%         \hline
%         -- &\#1 \#4 & \makecell[l]{Chemal \textit{et al.}\cite{Chemali2017},\\Jiao \textit{et al.}~\cite{jiao_gru-rnn_2020}} & $\sum\limits^N_{t=0} \frac{1}{2} (SoC_t-\hat{SoC_t})^2$ \\
%         \hline
%         \makecell[l]{Mean Average\\ Percentage Error} &\#3 & Mamo \textit{et al.}~\cite{mamo_long_2020} & $\frac{100}{N}\sum\limits^N_{t=0}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
%         \hline
%         -- &\#2 & Xiao \textit{et al.}~\cite{xiao_accurate_2019} & $\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}{N}$ \\
%         \hline
%         \makecell[l]{Mean Average\\ Squared Error} &\#5 \#6 & \makecell[l]{Javid \textit{et al.}~\cite{javid_adaptive_2020},\\Zhang \textit{et al.}~\cite{zhang_deep_2020}} & $\sum\limits^N_{t=0} \sqrt{\frac{1}{n} (SoC_t-\hat{SoC_t})^2}$ \\
%         \hline\hline
%     \end{tabular}
%     }
% \end{table*}

% $ L = \sum\limits^N_{t=0} \frac{1}{2} (SoC_t-\hat{SoC_t})^2$ ~\cite{Chemali2017},~\cite{jiao_gru-rnn_2020} \\
% $ L = \sum\limits^N_{t=0} \sqrt{\frac{1}{n} (SoC_t-\hat{SoC_t})^2}$ ~\cite{javid_adaptive_2020}~\cite{zhang_deep_2020} \\
% $ L = (\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2)N$~\cite{xiao_accurate_2019} \\
% $ L = |SoC_t-\hat{SoC_t}|$ \\
% $ L = \frac{100}{N}\sum\limits^N_{t=0}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$~\cite{mamo_long_2020} \\

% \ebd{table}
% $MAE = \frac{1}{N}\sum\limits^N_{t=1} |SoC_t-\hat{SoC_t}|$ \\
% $MAPE = \frac{100}{N}\sum\limits^N_{t=1}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
% $RMSE = \sqrt{\frac{1}{N}\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}$ \\
% $R^2 = 1-\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}
%               {\sum\limits^N_{t=1}(SoC_t-M_{SoC})^2}$ - $M_{SoC}$ is the mean SoC \\
% $Error = \frac{SoC_t-\hat{SoC_5}}{SoC_t}$ - Not decided yet
% \subsubsection{Callbacks}
% For the stateless model, the method for resetting a State of the model has been derived with the Callback function of:
% Another Algorithm. \\
% \begin{lstlisting}[language=Python]
% class ResetCallback(tf.keras.callbacks.Callback):
%     reset_steps : int = 500
%     i_counter   : int = 0
%     def __init__(self):
%         self.i_counter = 0

%     def on_batch_begin(self, batch, logs=None):
%         if (self.i_counter % self.reset_steps) == 0:
%             self.model.reset_states()
%             self.i_counter = 0
%         self.i_counter += 1
% \end{lstlisting}
% \begin{enumerate}
%     \item Checkpoint saving only the best one based on smalest rmse
%     \item Tensorboard tmp folder
%     \item Early stopping is applyable
%     \item Procedure of training epoch by epoch or algorithm for offline and online.
% \end{enumerate}
% \subsubsection{Callbacks}
% \begin{itemize}
%     \item Checkpoint \\
%     \item Tensorboard \\
%     \item nanTerminate \\    
% \end{itemize}
% \subsubsection{Training and validation Loop??}
% \begin{itemize}
%     \item validation over what data? - single cycle \\
% \end{itemize}
% \subsubsection{Testing procedures??}
% \begin{itemize}
%     \item Converting model to the TF-Lite usage for TPU processor \\
%     \item 2 profiles - two cycle test \\
% \end{itemize}

%
%
% \subsubsection{Metrics}
% \textcolor{red}{Generally, Losses start form 0, metrics from 1. Articles which did overwise were assumed to be mistaken.}
% \textcolor{red}{Similar to loses and reasons why we have chosen those as our criteria.} \\

% Metrics functions used through out the research
%
%! References on who used.
%! varied metric used across articles, we will used 3.
Metrics functions act as user evaluation criteria to assess the performance of the trained model during both fitting and validation processes.
Although some papers relied on different evaluation criteria, for this research, metrics were unified with several equations provided in \mbox{Table~\ref{tab:metrics}}.
Thus, the same criteria will be used to compare the models' efficiency against each other.
\begin{table}[htbp]
    \renewcommand{\arraystretch}{1.3}
    \caption{Model's metrics functions}
    \centering
    \label{tab:metrics}
    \resizebox{\columnwidth}{!}{
    \begin{tabular}{l c}
        \hline\hline \\[-3mm]
        Function Name & Equation \\ 
        \hline
        \makecell[l]{Mean Absolute\\ Error} &  $\frac{1}{N}\sum\limits^N_{t=1} |SoC_t-\hat{SoC_t}|$ \\
        % \hline
        % Mean Average Percentage Error & $\frac{100}{N}\sum\limits^N_{t=1}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
        \hline
        \makecell[l]{Root Mean\\ Square Error} & $ \sqrt{\frac{1}{N}\sum\limits^N_{t=1} \left(SoC_t-\hat{SoC_t} \right)^2}$ \\
        \hline
        \makecell[l]{$R^2$ : $M_{SoC}$ is\\ the mean SoC} & $1-\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}
                {\sum\limits^N_{t=1}(SoC_t-M_{SoC})^2}$ \\
        \hline\hline
        % Error : Not decided if useful yet & $ \frac{SoC_t-\hat{SoC_5}}{SoC_t}$
    \end{tabular}
    }
\end{table}

% Concluding purpose of the training and ultimate goad of getting so creative
%
% Overall, the primary criteria of the research are the ability to fit the training data, avoiding overfits, constant reasonably short retraining potential, susceptibility to undetermined conditions and final memory and processing usage.
% Ultimately, all models will be compared with their successes and faults to determine a path for potential improvement.
