\subsection{Dataset description and generator} \label{subsec:dataset}
% Windowing technique, batching and normalisation. **Figure 4**
%
A recurrent neural network is a subclass of NN which has proven effective in weather or stock price forecasting.
This method learns by recognising a pattern within a sequential data input, thus predicting the future outcome.
% It makes the following approach applicable to almost any type of time series-dependent problem~\cite{anton_battery_2013}.
% The two vectors or matrices define the Input and Output Layers.
Two vectors or matrices define the inputs and outputs of a model.
The general description of a single input X is in \mbox{Equation~(\ref{eq:x-matrix})} and an output Y in \mbox{Equation~(\ref{eq:y-matrix})}.
$V, I$, and $T$ represent voltage (V), current (A), and temperature (\textdegree{}C), respectively, as input features, and $SoC$ is the fraction of state of charge (between 0 and 1) as the output.
All samples are equally time-distributed, and $t$ represents the number of input time steps at a time.
% Since the sampling rate during the charging process is less than during discharge, the missing step times were resampled.
Considering the characteristics of a constant current and constant voltage charging, this workaround should not cause any training issues.
\begin{equation}
    X \left (n \right ) =
    \begin{Bmatrix}
        V \left (0 \right ) & V \left (1 \right ) & ... & V \left (t \right )\\
        I \left (0 \right ) & I \left (1 \right ) & ... & I \left (t \right )\\
        T \left (0 \right ) & T \left (1 \right ) & ... & T \left (t \right )\\
    \end{Bmatrix}
    \label{eq:x-matrix}
\end{equation}
\begin{equation}
    Y \left (n \right ) =
    \begin{Bmatrix}
        SoC \left (t \right )
    \end{Bmatrix}
    \label{eq:y-matrix}
\end{equation}

% Describe the size of the history selection.
%
%! Find the picture and redo the title with "various"
% For dataset generation for training and testing purposes, data is combined in a 3-dimensional matrix using windowing techniques as per \mbox{Figure~\ref{fig:Windowing3f}}, where $s$ represents the step between each window, less than the number of input time steps.
% \if{thesis}
\ifthenelse {\boolean{thesis}} {} {
    Both stateful and stateless methods rely on the input samples' quality and length.
    Chemali et al.~\cite{Chemali2017} researched the impact of the history of input samples: the longer the period of input readings, the better the accuracy the model produced and the longer it took to compute.
    The research results are plotted in \mbox{Figure~\ref{fig:chemali-accuracy}}, outlining the root square parabola behaviour regarding the size of the history compared with errors in the prediction.
    The optimum size of the windows for stateless models obtained by Chemali et al.~\cite{Chemali2017} was 500 samples.
    Any more significant matrices led to an increase in computation time but an insignificant difference in performance; thus, 500 was used in this work.
}
\begin{figure}[H]
    % \centering
    \includesvg[width=10cm]{II_Body/images/chemali_accuracy.svg}
    \caption{SoC estimation accuracy of LSTM-RNN with various network depths in time obtained by Chemali et al.~\cite{Chemali2017} in a plot representation.}
    \label{fig:chemali-accuracy}
\end{figure}

% Windowing technique, batching and normalisation. **Figure 4**
%
To generate datasets for training and testing purposes, data were combined in a three-dimensional matrix using windowing techniques, as per \mbox{Figure~\ref{fig:Windowing3f}}.
These figures provide an example of the stateless model input data visualisation, where the step between each window $s$ was less than the number of input time steps.
All stateful models used the same windowing technique to keep data generation simple, with a sample size of 1.
The state reset for stateful models occurred at the end of every cycle, allowing for a batching mechanism to be implemented to speed up the training process.
For example, 12 discharge process datasets with a similar voltage, current, and state of charge, but different temperatures at a time $t$, could be treated as a single batch.
The statefulness of a model preserved the state at index $i$ to the same index in the next batch~\cite{zhu_statefulnes_tfdocs_2020}.
In addition, the normalisation technique according to the mean and standard deviation, based on the entire training data of all three input features, was applied to speed up the fitting process.
%%%
% For the process of training for Stateful models speeding up, all data can be separated by batches \textit{b}, for each batch representing a different temperature category, creating a 4-dimensional matrix.
% \textit{f} defines the number of features as per Voltage, Current and Temperature.
% \textit{b} and \textit{s} were kept as the size of 1 for all scenarios to minimise resource consumption and simplify performance validation.
% Equation~\ref{eq:XY-shape} represent the final shape for Input data.
% The output shape for both Stateful and Stateless models remains the same.
% \begin{equation}
%     \begin{split}
%         X_{shape} = (n, b, t, f) & => (n, t, f) - Stateless \\
%                                  & => (n, 1, f) - Stateful \\
%         Y_{shape} = (n, b, 1) \ \ &=> (n, 1)
%     \end{split}
%     \label{eq:XY-shape}
% \end{equation}
%One of the first methods of SoC Estimation by Chemali \textit{\textit{et al.}}~\cite{Chemali2017} proved that 500 input time-steps produced the most efficient results, any further grow made insignificant impact.
%The same value used in this article. \\

%
%
%The value of SoC has been calculated from the difference between charge and discharge capacity.
%Values were rounded to 2 decimal places and kept in the range of 0 and 1 using min-max normalisation to reduce error in estimation.
%Since there is no way to directly obtain the battery's accurate charge practically during a run, output SoC is excluded from the time-series model's input feature, unlike any classic examples of usage Time-Series estimations.
%The trainable model has to distribute weights across inputs and still develop a charge's close estimate.
%All input samples were taken through normalisation by the mean and standard deviation across all input files with the same values, simplifying model weights acquiring and speeding up the training process.
%It is important to note that the normalisation values of mean and standard deviation from a training dataset are applied to validation and testings sets.
% TODO: Describe averaging