\subsection{Hyper-parameters selection} \label{subsec:l-rate}
% MEthods and their optomisers
%
The reviewed articles tended to use constant hyper-parameters values for models and optimisers, including learning rate.
However, each selected different values based on experiments and observations, somewhere in the range 0.001, the standard framework provided, down to 0.0005(0.0001) to make a smooth stepping and minimise potential missing the optimum minimum.
A detailed overview of each method has been marked in \mbox{Table ~\ref{tab:experiment}}.

% Recovery from faults.
%
As a result, a separate learning rate scheduler has been used instead of applying each learning rate separately for every model.
Therefore, a stepping learning rate algorithm has been used for every optimiser over the number of epochs the model went through.
Considering that all attempts to train the estimator tended to overfit, the value has been reduced by half each time model attempted to recover from overshoot.
However, it did not affect the follow-up training, meaning that the learning rate always followed the stick degradation curve as per Figure~\ref{fig:l_rate_progress}.
Epochs 26 and further indicate the training attempt to recover from the overfitting state on an LSTM model, reducing the rate in several dozen attempts, never reaching zero.
Once the model fails to improve the accuracy, it is considered the most optimum fit to initiate early stopping.
\begin{figure}[ht]
    \centering
    \includesvg[width=\linewidth]{II_Body/images/l_rate.svg}
    \caption{Learning rate degradation.\textcolor{red}{Sizing is bad for JA, need to decrase scaling}}
    \label{fig:l_rate_progress}
\end{figure}

% Betas and Epsilon instead on every optimiser page
%
The betas and epsilon did not go through any similar optimisation process due to a lack of documented training attempts to improve fitting over State of Charge estimation on Lithium-Ion batteries.
As a result, they were kept constant for all trained models, except SGD with Momentum as per \mbox{Table~\ref{tab:uni-hyperparams}}.
Since Stochastic Gradient Descent does not use any other hyper-parameters except learning rate and single beta constant, its' value was set to $\beta_1 = 0.8$
\begin{table}[htbp]
  \renewcommand{\arraystretch}{1.3}
  \caption{Optimisers Hyper-Parameters}
  \centering
  \label{tab:uni-hyperparams}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{ l l l l l l }
      \hline\hline \\[-3mm]
      $\alpha$ & $\beta_1 $ & $\beta_2$ & $\beta_3$ &  $\epsilon$ \\
      \hline
      Linear         &  &  &  & \\% 0.0000001
      Scheduler      & $0.9$ & $0.999$ & $0.999$ &$10^{-8}$ \\% 0.0000001
      (0.001-0.0005) &  &  &  & \\% 0.0000001
      \hline\hline
  \end{tabular}
  }
\end{table}

% Layers and neurons
%
All reviewed articles used a constant number of layers and neurons to conduct the experiments.
None have provided any reasoning for the selections or results from other attempted experiments without changing the technique.
The least time-consuming method evaluated the most promising combination of the number of layers and neurons to create the best similar circumstances for all methods.
The most promising candidates were taken through a dozen similar attempts to get an average result and produce several selection criteria for later use.

% Evaluation process of layers and neurons to get best set
%
The ranges between 1 to 3 layers and incremental combinations of neurons from 131 to 524 were used to determine the most optimum solution.
Any values higher or lower did not provide worthy outputs.
In a case with multiple layers, the number of neurons gets evenly distributed per layer, narrowed to the lowest integer.
For example, three layers at 131 neurons would represent each LSTM or GRU layer containing only 43.
As a result, the following Table, sorted by applicable criteria, has been selected.
Each model made three attempts per each profile, resulting in a total of 135 trained models to evaluate.
