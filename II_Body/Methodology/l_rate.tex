\subsection{Hyperparameters selection} \label{subsec:l-rate}
% Methods and their optimisers
%
%! Five stated models
The reviewed articles that the chosen models for testing were based on used constant hyperparameter values for models and optimisers, including learning rate.
However, each selected different values based on experiments and observations, somewhere in the range of 0.001 (the standard framework provided) down to 0.0001, to make a smooth stepping and minimise potentially missing the optimum minimum.
% A detailed overview of each method has been marked in \mbox{Table ~\ref{tab:experiment}}.

% Recovery from faults.
%
% Put a pseudo code for scheduler degradation.
% elif type == 'linear':
%   return linspace(0.001, 0.0001, 100)[epoch]
%   step = (stop - start) / (num - 1)
% for i in range(num):
%   yield start + step * i
% \begin{algorithm}\captionsetup{labelfont={sc,bf}, labelsep=newline}
%   \caption{Linear Scheduler implementation. Equivalent of \textit{linspace()} fn}
%   \begin{algorithmic}[1]
%     \STATE \textbf{Starting value} \\ $start\gets \textit{initial Learning rate}$\\
%     \STATE \textbf{End Value} \\ $stop\gets \textit{final Learning rate}$\\
%     \STATE \textbf{Number of iterations/Epochs} \\ $N = \textit{Epochs}$\\
%     \STATE \textbf{Calculate step between starting and end values, \\ including final learning rate} \\ $step = (stop - start) / (N - 1)$\\
%     \WHILE{$epoch \leq N$}
%     \STATE $\alpha \gets start + step * epoch$ \\ \COMMENT{Yield learning rate to an optimiser}
%     \ENDWHILE
%   \end{algorithmic}
%   \label{alg:scheduler}
% \end{algorithm}
% alpha_1 = alpha + delta
% Define delta degradation of learning rate
% Delta wash chosen to go from init to end. Decrement.
\begin{equation}
  \begin{split}
      IF \ AN \ & OVERFIT: \\
      \hat{\alpha} &= \frac{\alpha }{2} \\
      ELSE: & \\
      \Delta &= \frac{stop - start}{N-1} \\
      \hat{\alpha} &= \alpha + \Delta \times epoch \\
      \label{eq:scheduler}
  \end{split}
\end{equation}
\begin{figure}[ht]
    \centering
    \includesvg[width=\textwidth]{II_Body/images/l_rate.svg}
    \caption{Learning rate degradation}
    \label{fig:l_rate_progress}
\end{figure}
% As a result, a separate learning rate scheduler has been used instead of applying each learning rate separately for every model.
In this work, a stepping learning rate algorithm has been used for every optimiser over the training course, Equation~(\ref{eq:scheduler}).
The implementation is equivalent to a \textit{linspace} function, where every epoch's learning rate $\hat{\alpha}$ gets calculated through \textit{start} and \textit{stop} variables, indicating boundaries of the rates' degradation, with $\Delta$ decrement through a total of \textit{N} number of iterations inclusive, and known $\alpha$ value.
%
% Like adaptive time stepping in Finite Element Analysis [ref] by Pr David Holmes
The learning rate is sequentially reduced following a standard stepped scheme.
However, an additional adaptive phase is introduced in this work.
While training error in subsequent epochs reduces, the stepped scheme is followed.
The adaptive scheme is deployed if the learning error increases from one epoch into the next.
Then, the rate is halved, and the same epoch is rerun.
If the training error remains above the error of the previous epoch, the learning rate continues to be reduced to a predetermined minimum.
The prior stepped scheme resumes if the error returns to a convergent value.
It is shown in Figure~\ref{fig:l_rate_progress}, where two adaptive schemes are applied and reconverged from, and a final ultimate adaptive decay ends the training.
%
This approach has been found to be superior in this work to other adaptive learning rate schemes, and is employed for all subsequent training.
%
% However, continuous learning rate reduction does not resolve the issue of overfitting or missing a minimum.
% The best approach identified in this work to save time and produce reliable results has been to enforce a model to stick toward a single minimum.
% In this case, the learning rate always follows the stick degradation curve as per Figure~\ref{fig:l_rate_progress}, without affecting the follow-up training with steep rate reduction upon failures.
% Since each model undergoes multiple attempts, there are good chances to explore a variety of different fitting scenarios rather than jumping between several minimal.
% Two subfigures on \mbox{Figure~\ref{fig:rollback}} demonstrate two different training attempts with and without a recovery algorithm.
% While subfigure~\ref{subfig:no-rollback} keeps the training process until either the number of 100 epochs gets reached or rapid overfit, the subfigure~\ref{subfig:no-rollback} continues reaching the selected minimal for as long as it can achieve a result.
% Considering that all attempts to train the estimator eventually tended to overfit, the learning rate's value has been reduced by half each time model recovers from overshoot, Equation~\ref{eq:scheduler}.
% Epochs 26 and further indicate the training attempt to recover from the overfitting state on an LSTM model, reducing the rate in several dozen attempts, never reaching zero.
% Once the model fails to improve the accuracy, it is considered the most optimum fit to initiate early stopping.
\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includesvg[width=\linewidth]{II_Body/images/history-bad.svg}
      \caption{Model training process with no rollbacks \\ }
      \label{subfig:no-rollback}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\textwidth}
      \centering
      \includesvg[width=\linewidth]{II_Body/images/history-good.svg}
      \caption{Model training process with a rollback if current error higher than previous}
      \label{subfig:rollback}
  \end{subfigure}
  \caption{Training process comparison with and without having a recovery algorithm}
  \label{fig:rollback}
\end{figure}

% Betas and Epsilon instead on every optimiser page
%
The betas and epsilon optimisers' constants did not go through any similar optimisation process due to a lack of documented training attempts to improve fitting over State of Charge estimation on Lithium-Ion batteries.
As a result, they were kept constant for all trained models except SGDw/M.
Since Stochastic Gradient Descent does not use any other hyper-parameters except the learning rate and the single beta constant, its value was set to $\beta_1 = 0.8$.
The learning rate limits of the scheduler and remaining hyperparameters used in all cases are summarised in \mbox{Table~\ref{tab:uni-hyperparams}}.
\ifthenelse{\boolean{thesis}}{
\begin{table}[htbp]
  \renewcommand{\arraystretch}{1.3}
  \caption{Optimiser Hyper-Parameters}
  \centering
  \label{tab:uni-hyperparams}
  \begin{tabular}{ l l l l l l }
      \hline\hline \\[-3mm]
      $\alpha$ & $\beta_1 $ & $\beta_2$ & $\beta_3$ &  $\epsilon$ \\
      \hline
      Linear         &  &  &  & \\% 0.0000001
      Scheduler      & $0.9$ & $0.999$ & $0.999$ &$10^{-8}$ \\% 0.0000001
      (0.001-0.0001) &  &  &  & \\% 0.0000001
      \hline\hline
  \end{tabular}
\end{table}
} {
\begin{table}[H]
  \caption{Optimiser Hyper-Parameters}
  \label{tab:uni-hyperparams}
  \newcolumntype{C}{>{\centering\arraybackslash}X}
  \begin{tabularx}{\textwidth}{ C C C C C C }
    \toprule
    \textbf{$\alpha$} & \textbf{$\beta_1 $} & \textbf{$\beta_2$} & \textbf{$\beta_3$} &  \textbf{$\epsilon$} \\
    \midrule
    Linear         &  &  &  & \\% 0.0000001
    Scheduler      & $0.9$ & $0.999$ & $0.999$ &$10^{-8}$ \\% 0.0000001
    (0.001-0.0001) &  &  &  & \\% 0.0000001
    \bottomrule
  \end{tabularx}
\end{table}
}
% Layers and neurons
%
The work performed by other authors from Table~\ref{tab:review} uses a constant number of layers and neurons to conduct the experiments.
Only a few have provided reasoning for the selections or results from other attempted experiments without changing the technique~\cite{jiao_gru-rnn_2020,mamo_long_2020}.
The least time-consuming method evaluated the most promising combination of the number of layers and neurons to create the best similar circumstances for all methods.
The most promising candidates were taken through a dozen similar attempts to get an average result, and produce several selection criteria for later use.
