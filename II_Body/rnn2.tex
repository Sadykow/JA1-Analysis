%
%   Managing input data
%
\subsection{Managing input data} \label{subsec:RNN}
As discussed in the introduction, Section~\ref{sec:Introduction}, the purpose of the article is to analyse several different RNN models, measure the performance, and determine the most promising direction for further enhancing Neural Network models' integration into an embedded device.
Although the perfect replication was impossible due to a lack of details and actual battery data used by different authors, all models were implemented based on the provided information.
All missing aspects were assumed based on ML's standard methods at the time of their writing.
Summarising Table~\ref{tab:review} and Literature Review, Table~\ref{tab:experiment} compiles models, which will be evaluated in this work.
It provides details of 7 different implementations, which vary in their structure and learning process but share the same training, validation, testing and performance measurement procedures.
\begin{center}
    \begin{table}[h]
    \caption{Testing models summary.}
    \label{tab:experiment}
\begin{tabular}{p{0.4cm}|p{3.4cm}|p{1.3cm}|p{1.0cm}|p{1.2cm}|p{1.5cm}|p{1.5cm}|p{3.0cm}}
    \hline
    \multirow{2}{ 4em }{№} &
    \multirow{2}{ 4em }{Structure \& Cells} &
    \multirow{2}{ 4em }{State$-$ } &
    \multicolumn{4}{ c|}{Optimiser Learning Rate} &
    \multirow{2}{ 4em }{Extension} \\
    \cline{4-7}
      &                       &         & Adam  & Nadam & SGDw/M & AdaMax &           \\
    \hline
    1 & 1 $\times$ LSTM (500) & $-$less & 0.001 &       &        &        &           \\
    2 & 1 $\times$ GRU (560)  & $-$less &       & 0.001 &        & 0.0005 & Ensemble  \\
    3 & 1 $\times$ LSTM (520) & $-$less & 0.001 &       &        &        & Attention \\
    4 & 2 $\times$ GRU (64)   & $-$full &       &       & 0.01   &        & Momentum (0.8)\\
    5 & 4 $\times$ GRU (64)   & $-$full & 0.001 &       &        &        &           \\
    6 & 1 $\times$ GRU (500)  & $-$less & 0.001 &       &        &        & Robust (RoAdam)\\
    \hline
\end{tabular}
    \end{table}
\end{center}

%
%
The structure and cell type define the number of layers of particular cell types with the total number of neurons evenly shared across layers.
The statefulness parameter describes the model ability to preserve its' current state for the next set of input parameters.
Based on that, the input amount of sample becomes flexible by the requirement of adding only a single sample at a time upon their arrival, instead than wait until the fixed amount has arrived.
The optimiser selection was based on the derivative calculation algorithms only.
Other alternatives, like differential evolution, are beyond the scope of research.
Extention defines the specific detail of the model, which distinguish it against the others.

%
%
Thus the specifics of each algorithmic aspect will be defined in the following: 2.1,2.2,2.3
\begin{itemize}
    \item Data shape for each state type
    \item Model structure and difference between GRU and LSTM
    \item Each optimisation algorithm and hyper-parameters selection
\end{itemize}

%
%
\subsection{Dataset description and generator} \label{subsec:dataset}
Recurrent Neural Network is a subclass of NN, which has proven effective in weather or stock prices forecasting.
This method learns by recognising a pattern within sequential data input, thus predicting the future outcome.
It makes the following approach applicable to almost any type of time series dependant problem~\cite{anton_battery_2013}.
Classical RNN consists primarily of three layers, represented in Figure~\ref{fig:RNN-structure}.
The Input and Output Layers defined by the two vectors or matrices.
The general description of a single input X and an output Y samples represented by Equations~\ref{eq:xy-matrix}.
$V, I, T, SoC$ represent Voltage (V), Current (A), Temperature (\textdegree{}C) as input features, and percentage of State of Charge (between 0 and 1) as output.
All samples are equally time distributed, and the t represents the number of input time steps at a time.
\begin{equation}
    \textbf{X} \left (n  \right ) = 
    \begin{Bmatrix}
        V \left (0  \right ) & V \left (1  \right ) & ... & V \left (t  \right )\\ 
        I \left (0  \right ) & I \left (1  \right ) & ... & I \left (t  \right )\\ 
        T \left (0  \right ) & T \left (1  \right ) & ... & T \left (t  \right )\\
    \end{Bmatrix}
    \\ \textbf{Y} \left (n  \right ) = 
    \begin{Bmatrix}
        SoC \left (t  \right ) 
    \end{Bmatrix}
\label{eq:xy-matrix}
\end{equation}

%
%
For dataset generation of training and testing purposes, data is combined in a 3-dimensional matrix using windowing techniques as per Figure~\ref{fig:Windowing}, where $s$ represents the step between each window, less than the number of input time steps.
The most optimum size of the windows for stateless models, obtained by Chemali et al.~\cite{Chemali2017}, will be 500 samples.
Any more significant matrices will lead to an increase in computation time but an insignificant difference in performance.
All stateful models used the same windowing technique to keep data generation simple but with a sample size of 1.
The state reset for stateful models happens at the end of every dataset, allowing a batching mechanism to speed up the training process.
For example, 12 datasets of discharge process with similar Voltage, Current and the State of Charge, but different temperatures at a time $t$ can be treated as a single batch.
The statefulness of a model preserves the state at index $i$ to the same index in the next batch [ref to the document will not hurt].
%%%
% For the process of training for Stateful models speeding up, all data can be separated by batches \textit{b}, for each batch representing a different temperature category, creating a 4-dimensional matrix.
% \textit{f} defines the number of features as per Voltage, Current and Temperature.
% \textit{b} and \textit{s} were kept as the size of 1 for all scenarios to minimise resource consumption and simplify performance validation.
% Equation~\ref{eq:XY-shape} represent the final shape for Input data.
% The output shape for both Stateful and Stateless models remains the same.
% \begin{equation}
%     \begin{split}
%         X_{shape} = (n, b, t, f) & => (n, t, f) - Stateless \\
%                                  & => (n, 1, f) - Stateful \\
%         Y_{shape} = (n, b, 1) \ \ &=> (n, 1)
%     \end{split}
%     \label{eq:XY-shape}
% \end{equation}
%One of the first methods of SoC Estimation by Chemali \textit{et al.}~\cite{Chemali2017} proved that 500 input time-steps produced the most efficient results, any further grow made insignificant impact.
%The same value used in this article. \\

%
%
The value of SoC has been calculated from the difference between charge and discharge capacity.
Values were rounded to 2 decimal places and kept in the range of 0 and 1 using min-max normalisation to reduce error in estimation.
Since there is no way to directly obtain the battery's accurate charge practically during a run, output SoC excluded from the time-series model's input feature, unlike in any classic examples of usage Time-Series estimations.
The trainable model has to distribute weights across inputs and still develop a charge's close estimate.
All input samples were taken through normalisation by the mean and standard deviation across all input files with the same values, simplifying model weights acquiring and speeding up the training process.
It is important to note that the normalisation values of mean and standard deviation from a training dataset are applied to both validation and testings sets.

%
%
The training process conducted through all datasets for a single battery testing profile and validated on a single cycle (less or around 20\% of the entire set).
Final tests for a model performance conducted against an entire set of two remaining profiles separately.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{II_Body/images/SoC-RNN.png}
    \caption{Universal structure of RNN for SoC estimation.}
    \label{fig:RNN-structure}
\end{figure}
\begin{landscape}
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.9\linewidth]{II_Body/images/Windowing3D-3.jpg}
        \caption{Data Windowing scheme. For visualisation purposes, the $s$-step has been used as 250 seconds, which is different from actual implementation. Initial index $i$ was kept as a value close to the beginning of the data, around zero.}
        \label{fig:Windowing}
    \end{figure}
\end{landscape}
%
%   Model Structure
%
\subsection{Model structure} \label{subsec:structure}
The general summary model structure is represented in Figure~\ref{fig:RNN-structure}, with three feature inputs and a single output.
The number of hidden layers will be dependant on the authors' implementation.
Since the output consists of only a single sample, it is defined by a fully connected layer - a dense layer with a single neuron.
In case if the article did not specify the number of neurons per layer, one of the following equation has been used to get the initial raw estimation~\cite{eckhardt_choosing_2018}:
\begin{equation}
    \begin{split}
        N_h &= \frac{N_s}{ a \left(N_i+N_o \right)} \ \ OR \ \ N_h = \frac{2}{3}\left(N_i+N_o \right) \\
        N_i &\rightarrow \text{Number of input neurons} \\
        N_o &\rightarrow \text{Number of output neurons} \\
        N_s &\rightarrow \text{Number of samples in training data set} \\
        \alpha &\rightarrow \text{An arbitrary scaling factor 2(5)-10}
    \end{split}
\end{equation}
%**Contrary with Hidden Layers as per their name, they obey only internally defined logic and connections.
%**That is why models are stable and reliable once created, cannot be changed, only retrainable with different data.
%A standard Hidden layer, consisting of fully connected neurons, called a Dense Layer.
%A standard Hidden layer, consisting of fully connected neurons and only one or none activation function, called a Dense Layer.
%Within each cell of a Dense layer lies a single activation function.

%An Output Layer gets created from Dense Layer and commonly with no Activation function.
%: \textit{Simple, Exponential or Rectified Linear; Sigmoid and Hyperbolic Tangent functions}. \\
% \begin{itemize}
%     \item \textit{linear} - Simple Linear function
%     \item \textit{elu} - Exponential Linear function
%     \item \textit{relu} - Rectified Linear unit function
%     \item \textit{sigmoind} - Sigmoid function $sigmoid(x) = 1/1(1+exp(-x)$
%     \item \textit{tanh} - Hyperbolic Tangent function $$
% \end{itemize}

%
%
There are several activation functions for those layers widely used in machine learning libraries for time series problems~\cite{amidi_cs_2018}.
For the SoC prediction problem, all authors used the same function.
It also was experimentally confirmed that the best one for all hidden layers is the hyperbolic tangent function, Equation~\ref{eq:tanh}.
The output layer does not use the same logic, and no activation gets applied.
A dropout layer technique with a 20\% cut-off has been applied to all hidden layers to prevent data overfitting over long training periods.
\begin{equation}
    tanh(x) = \frac{sinh(x)}{cosh(x)}=\frac{e^x-e^{-x}}{e^x+e^{-x}}
    \label{eq:tanh}
\end{equation}
%
%
%The common usage of Dense layer in this paper is the Output Layer with no activation functions and with single neuron representing single output SoC value.
%To apply multiple different without overcompicating neurons a multiple bunch of layers with different or the same number of neurons or activations functions gets applied.
%The more Hidden layers network contains - the deeper and computationally complex a network becomes, which also referred as Deep Neural Networks. 

% Choosing the number of neurons for a first Hidden Layer does not have a golden rule.
% For this article, following formula helps to make a good initial estimate based on Number samples, Inout and Output.\\
% \textbf{Equation}\\
% A common practice is to narrow each following Neurons by 2 from previous one, to make a more accurate capture.
% To avoid overfitting data, except for data normalization and Input sample shuffeling in stateles models, a Dropout technue gets applied.
% Tensorflow has an intermnal implementation of Dropoout for GRU and LSTM layers. A value of 0.2 (20\%) applied to all RNN layers to minimizze overfitting possibility.

%
% Requires editing below
The efficiency of an RNN over time series problem is defined by the ability of the neurons to store memory as an internal state.
With time, the memory of long passed samples may fade away.
The problem refereed as vanishing gradient, when the value to update network weights shrinks as it propagates through time~\cite{rasifaghihi_predictive_2020}.
Long-term dependencies do not get captured since layers with a slight gradient do not significantly affect due to insufficient weight change~\cite{rasifaghihi_predictive_2020,hochreiter_vanishing_1998}.
The more complicated structures of neurons tend to solve that problem.
% The gradient is the value used to update Neural Networks’ weight.
%"Therefore, layers that get a small gradient do not learn and they cause the network to have short-term memory."
% https://www.bioinf.jku.at/publications/older/2304.pdf
%"With gradient based learning methods the current error signals has to 'flow back in time' over the feedback connections to past inputs for building up an adequate input storage. Long-term dependancies are hard to learn because of insufficient weight changes."

%
%
There are two commonly used Recurrent Neural Network, which utilises memory cells: Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) with possible extensions implemented by articles' authors.
The GRU implementation will be used as a stateful technique by preserving a state from batch to batch with a single sample at a time.
The LSTM will take a stateless approach, providing a fixed number of timestamps at a time.
%Implementation of the model based on simple RNN using multi layer Dense(X) networks.~\cite{lees2010theoretical}
%A very basic version of Recurrent Neural Network consist of very basic layers with some amount of neurons.

% \subsubsection{Implementation}
%     The input data for a network has been created using Windowing technique, where \textbf{216k} sample of battery data, consisting of State Of Charge only were separated on 500 sample windows.
%     As a results, model outputs a single sample as SoC at next Time Step. Using Tensorflow library and calculating number of Neurons using recomended formula \\
%     \textbf{THIS IS THE BEST PLACE FOR IT}. No other places suites as much.
%     The strcuture of the model ha\subsection{Training and Validation}

%s the following form. (Few Dense Layers+Dropount).
%     A Dropout layer has been used to prevent data overfitting. The selection of activation functions has been done through the properties of the data, which model has to fit in and multiple trials.
% \subsubsection{Observation}
%     A simple Recurrent Neural Network has proven itself effective with simple Linear problems. However, with battery state of charge it is unable to capture complicated features like transition between Discharge and Charge or process of Constant-Voltage Constant-Current charging. In the application of battery utilisation inside Electrical Vehichle, this approach can be used only with some additional logic, such as Kalman Filters.
%     The best approach is to introduce more information about battery state and use more complicated version of Time Series capable to memorise feature with time, such as LSTM~\ref{sec:LSTM} and GRU~\ref{sec:GRU}.
%     The results of the prediction discussed in Section~\ref{sec:results}.
%\subsection{Stateful Gradient Recurrent Unit (GRU) Algorithms and Implementations} \label{subsec:GRU}
\input{./II_Body/GRU/gru} 
%\input{./II_Body/GRU/gru-nadam}
%\subsection{Stateless Long Short-Term Memory (LSTM) Algorithms and Implementations} \label{subsec:LSTM}
\input{./II_Body/LSTM/lstm}
%\input{./II_Body/LSTM/lstm-attention}
\subsection{Optimisers} \label{subsec:optimisers}
The optimisation algorithm aims to define a way to minimise the difference between model prediction and actual values, using one of the selected Loss functions~\ref{subsub:losses}.
The following section breaks down several methods selected by chosen authors in the growing complexity.
\input{./II_Body/Optimisers/optimisers}
\subsection{Battery Data for training and validation}
Model training has been conducted over Lithium Ion battery cycling data, obtained from CALCE university~\cite{noauthor_calce_2017}.
The SoC value calculated from following equation where $C$ represent Charge and $D$ Discharge Capacities in $Ah$.
Table~\ref{tab:battery} highlights selected battery characteristics.
The Battery Cycling data in a Battery testing chamber were stored as Excel spreadsheets over temperature range 0 and 50 degrees.
Each testing cycle contains 3 testing profiles: Dynamic stress test (DST), (US06) and the Federal urban driving schedule (FUDS).
The temperatrue step 10 degrees with tolerance around 1 degrees.
The range of 20 to 50 was used as training and testing dataset.
Each method went through a single cycling profile and later tested against the other two, as per Mamo \textit{et. al.} \cite{mamo_long_2020} research.
\begin{equation}
    \begin{split}
        \hat{SoC} &= MinMax(C-D) \\
        SoC &= \frac{round(100*\hat{SoC})}{100}
    \end{split}
\end{equation}
\begin{table}[ht]
    \centering
    \caption{Battery characteristics}
    \label{tab:battery}
    \begin{tabular}{ p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm}   }
        \hline
        % \hline
        % \multicolumn{5}{c}{Battery Characteristics} \\
        %\hline
        Brand name & Battery Weight & Nominal  & Nominal & Charge/discharge\\
                   &                & Capacity & Voltage & cut-off voltage \\
        %\hline
        \hline
        %A456 \\ (former A123) & 76g+-1g & 2.3Ah & 3.2V & 3.65V, 2.0 V\\
        A123 (2012) & 76g+-1g & 2.3Ah & 3.2V & 3.65V, 2.0 V\\
        %\hline
        \hline
    \end{tabular}
\end{table}
\subsubsection{Losses}~\label{subsub:losses}
\textcolor{red}{Small description or purpose of the loss functions or what symbol they replace. Redo it to fit whatever I was using.} \\
%\begin{center}
    \begin{tabular}{ l l c }
        \hline
        Name & Articles & Equation \\ 
        \hline
        Absolute Error & Song \textit{et al.}~\cite{song_lithium-ion_2018} & $|SoC_t-\hat{SoC_t}|$ \\
        \hline
        | & Chemal \textit{et al.}\cite{Chemali2017}, Jiao \textit{et al.}~\cite{jiao_gru-rnn_2020} & $\sum\limits^N_{t=0} \frac{1}{2} (SoC_t-\hat{SoC_t})^2$ \\
        \hline
        Mean Average Percentage Error & Mamo \textit{et al.}~\cite{mamo_long_2020} & $\frac{100}{N}\sum\limits^N_{t=0}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
        \hline
        | & Xiao \textit{et al.}~\cite{xiao_accurate_2019} & $\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}{N}$ \\
        \hline
        Mean Average Squared Error & Javid \textit{et al.}~\cite{javid_adaptive_2020}, Zhang \textit{et al.}~\cite{zhang_deep_2020} & $\sum\limits^N_{t=0} \sqrt{\frac{1}{n} (SoC_t-\hat{SoC_t})^2}$ \\
        \hline
    \end{tabular}
%\end{center}
\newline
% $ L = \sum\limits^N_{t=0} \frac{1}{2} (SoC_t-\hat{SoC_t})^2$ ~\cite{Chemali2017},~\cite{jiao_gru-rnn_2020} \\
% $ L = \sum\limits^N_{t=0} \sqrt{\frac{1}{n} (SoC_t-\hat{SoC_t})^2}$ ~\cite{javid_adaptive_2020}~\cite{zhang_deep_2020} \\
% $ L = (\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2)N$~\cite{xiao_accurate_2019} \\
% $ L = |SoC_t-\hat{SoC_t}|$ \\
% $ L = \frac{100}{N}\sum\limits^N_{t=0}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$~\cite{mamo_long_2020} \\
\subsubsection{Metrics}
\textcolor{red}{Generally, Losses start form 0, metrics from 1. Articles which did overwise were assumed to be mistaken.}
\textcolor{red}{Similar to loses and reasons why we have chosen those as out criterias.} \\
\begin{tabular}{l c}
    \hline
    Mean Average Error &  $\frac{1}{N}\sum\limits^N_{t=1} |SoC_t-\hat{SoC_t}|$ \\
    % \hline
    % Mean Average Percentage Error & $\frac{100}{N}\sum\limits^N_{t=1}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
    \hline
    Root Mean Square Error & $ \sqrt{\frac{1}{N}\sum\limits^N_{t=1} \left(SoC_t-\hat{SoC_t} \right)^2}$ \\
    \hline
    $R^2$ : $M_{SoC}$ is the mean SoC & $1-\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}
              {\sum\limits^N_{t=1}(SoC_t-M_{SoC})^2}$ \\
    % \hline
    % Error : Not decided if useful yet & $ \frac{SoC_t-\hat{SoC_5}}{SoC_t}$
\end{tabular}
\\
% $MAE = \frac{1}{N}\sum\limits^N_{t=1} |SoC_t-\hat{SoC_t}|$ \\
% $MAPE = \frac{100}{N}\sum\limits^N_{t=1}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
% $RMSE = \sqrt{\frac{1}{N}\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}$ \\
% $R^2 = 1-\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}
%               {\sum\limits^N_{t=1}(SoC_t-M_{SoC})^2}$ - $M_{SoC}$ is the mean SoC \\
% $Error = \frac{SoC_t-\hat{SoC_5}}{SoC_t}$ - Not decided yet
% \subsubsection{Callbacks}
% For stateless model, the method for reseting a State of model has been derived with Callback function of:
% Another Algorithm. \\
% \begin{lstlisting}[language=Python]
% class ResetCallback(tf.keras.callbacks.Callback):
%     reset_steps : int = 500
%     i_counter   : int = 0
%     def __init__(self):
%         self.i_counter = 0

%     def on_batch_begin(self, batch, logs=None):
%         if (self.i_counter % self.reset_steps) == 0:
%             self.model.reset_states()
%             self.i_counter = 0
%         self.i_counter += 1
% \end{lstlisting}
% \begin{enumerate}
%     \item Checkpoint saving only the best one based on smalest rmse
%     \item Tensorboard tmp folder
%     \item Early stopping is applyable
%     \item Procedure of training epoch by epoch or algorithm for offline and online.
% \end{enumerate}
\subsection{Hardware and Software} \label{subsec:soft}
\textcolor{red}{Need to systemise information better} \\
Tensorflow 2.4.0 with Python 3.9.1 compiled from source with bazel 3.1.0, with packages of Addons version 0.18.* for $R^2$ and Probability 0.* for DE algorithm complied from the source. \\
Attention Layer has been reimpolemented from provided source code.\textcolor{red}{Good luck finding the link}
Tensorboard used to observe progress as it goes and keep records to analyze training later.
Hardware Intel CPU with Graphical Processor with Compute Capability Score of 7.5, CUDA version 11.1.
Multilayer model flaged with Return sequences boolean. Statefulnes define with Stateful boolean. The dropout affected as separate layer or parameter. With Stateful on, standard Inpust Layer shape requires to be follower Bacthed Input shape. It can be done kwarg parameter to the GRU or LSTM layer or kept as a separate Input Layer to allow precompilation before starting model fitting. Following code snipest demonstrated the difference. \\
\textcolor{red}{I am going to use the TPU processor to measure time it takes to run each model.} \\
\begin{table}[ht]
    \centering
    \caption{Software details}
    \label{tab:my_label}
    \begin{tabular}{p{3.0cm}|c|c|c|c|c}
        Version & Python version & Compiler & Build tools & CUDA version & Compute Score\\
        Tensorflow-2.4.0, TF-Addons 13.0, TF-Probability 0.12.1 & 3.9.1 & gcc 9.3.0 & Bazel 3.1.0 & 11.1 & 7.5\\
        \hline
        Tensorflow-2.2.0 & 3.8.* & gcc 7.3.1 & Bazel 2.0.0 & CPU & 3.98MHz*
    \end{tabular}
\end{table}
\begin{table}[]
    \centering
    \caption{Chip-on-Board Hardware selection}
    \label{tab:my_label}
    \begin{tabular}{c|c|c|c}
        Device & Specification & Source & Software setup \\
        \hline
        Coral TPU & (Specs) & (Link to the details) & (Link to libedgetpu.so.1) \\
        \hline
        Raspberry Pi 4 (Active cooling) & 
    \end{tabular}
\end{table}