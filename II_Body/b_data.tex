Model training has been conducted over Lithium Ion battery cycling data, obtained from CALCE university~\cite{noauthor_calce_2017}.
The SoC value calculated from following equation where $C$ represent Charge and $D$ Discharge Capacities in $Ah$.
Table~\ref{tab:battery} highlights selected battery characteristics.
The Battery Cycling data in a Battery testing chamber were stored as Excel spreadsheets over temperature range 0 and 50 degrees.
Each testing cycle contains 3 testing profiles: Dynamic stress test (DST), (US06) and the Federal urban driving schedule (FUDS).
The temperatrue step 10 degrees with tolerance around 1 degrees.
The range of 20 to 50 was used as training and testing dataset.
Each method went through a single cycling profile and later tested against the other two, as per Mamo \textit{et. al.} \cite{mamo_long_2020} research.
\begin{equation}
    \begin{split}
        \hat{SoC} &= MinMax(C-D) \\
        SoC &= \frac{round(100*\hat{SoC})}{100}
    \end{split}
\end{equation}
\begin{table}[ht]
    \centering
    \caption{Battery characteristics}
    \label{tab:battery}
    \begin{tabular}{ p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm} p{2.7cm}   }
        \hline
        % \hline
        % \multicolumn{5}{c}{Battery Characteristics} \\
        %\hline
        Brand name & Battery Weight & Nominal  & Nominal & Charge/discharge\\
                   &                & Capacity & Voltage & cut-off voltage \\
        %\hline
        \hline
        %A456 \\ (former A123) & 76g+-1g & 2.3Ah & 3.2V & 3.65V, 2.0 V\\
        A123 (2012) & 76g+-1g & 2.3Ah & 3.2V & 3.65V, 2.0 V\\
        %\hline
        \hline
    \end{tabular}
\end{table}
\subsubsection{Losses}~\label{subsub:losses}
\textcolor{red}{Small description or purpose of the loss functions or what symbol they replace. Redo it to fit whatever I was using.} \\
%\begin{center}
    \begin{tabular}{ l l c }
        \hline
        Name & Articles & Equation \\ 
        \hline
        Absolute Error & Song \textit{et al.}~\cite{song_lithium-ion_2018} & $|SoC_t-\hat{SoC_t}|$ \\
        \hline
        | & Chemal \textit{et al.}\cite{Chemali2017}, Jiao \textit{et al.}~\cite{jiao_gru-rnn_2020} & $\sum\limits^N_{t=0} \frac{1}{2} (SoC_t-\hat{SoC_t})^2$ \\
        \hline
        Mean Average Percentage Error & Mamo \textit{et al.}~\cite{mamo_long_2020} & $\frac{100}{N}\sum\limits^N_{t=0}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
        \hline
        | & Xiao \textit{et al.}~\cite{xiao_accurate_2019} & $\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}{N}$ \\
        \hline
        Mean Average Squared Error & Javid \textit{et al.}~\cite{javid_adaptive_2020}, Zhang \textit{et al.}~\cite{zhang_deep_2020} & $\sum\limits^N_{t=0} \sqrt{\frac{1}{n} (SoC_t-\hat{SoC_t})^2}$ \\
        \hline
    \end{tabular}
%\end{center}
\newline
% $ L = \sum\limits^N_{t=0} \frac{1}{2} (SoC_t-\hat{SoC_t})^2$ ~\cite{Chemali2017},~\cite{jiao_gru-rnn_2020} \\
% $ L = \sum\limits^N_{t=0} \sqrt{\frac{1}{n} (SoC_t-\hat{SoC_t})^2}$ ~\cite{javid_adaptive_2020}~\cite{zhang_deep_2020} \\
% $ L = (\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2)N$~\cite{xiao_accurate_2019} \\
% $ L = |SoC_t-\hat{SoC_t}|$ \\
% $ L = \frac{100}{N}\sum\limits^N_{t=0}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$~\cite{mamo_long_2020} \\
\subsubsection{Metrics}
\textcolor{red}{Generally, Losses start form 0, metrics from 1. Articles which did overwise were assumed to be mistaken.}
\textcolor{red}{Similar to loses and reasons why we have chosen those as out criterias.} \\
\begin{tabular}{l c}
    \hline
    Mean Average Error &  $\frac{1}{N}\sum\limits^N_{t=1} |SoC_t-\hat{SoC_t}|$ \\
    % \hline
    % Mean Average Percentage Error & $\frac{100}{N}\sum\limits^N_{t=1}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
    \hline
    Root Mean Square Error & $ \sqrt{\frac{1}{N}\sum\limits^N_{t=1} \left(SoC_t-\hat{SoC_t} \right)^2}$ \\
    \hline
    $R^2$ : $M_{SoC}$ is the mean SoC & $1-\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}
              {\sum\limits^N_{t=1}(SoC_t-M_{SoC})^2}$ \\
    % \hline
    % Error : Not decided if useful yet & $ \frac{SoC_t-\hat{SoC_5}}{SoC_t}$
\end{tabular}
\\
% $MAE = \frac{1}{N}\sum\limits^N_{t=1} |SoC_t-\hat{SoC_t}|$ \\
% $MAPE = \frac{100}{N}\sum\limits^N_{t=1}|\frac{SoC_t-\hat{SoC_t}}{SoC_t}|$ \\
% $RMSE = \sqrt{\frac{1}{N}\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}$ \\
% $R^2 = 1-\frac{\sum\limits^N_{t=1}(SoC_t-\hat{SoC_t})^2}
%               {\sum\limits^N_{t=1}(SoC_t-M_{SoC})^2}$ - $M_{SoC}$ is the mean SoC \\
% $Error = \frac{SoC_t-\hat{SoC_5}}{SoC_t}$ - Not decided yet
% \subsubsection{Callbacks}
% For stateless model, the method for reseting a State of model has been derived with Callback function of:
% Another Algorithm. \\
% \begin{lstlisting}[language=Python]
% class ResetCallback(tf.keras.callbacks.Callback):
%     reset_steps : int = 500
%     i_counter   : int = 0
%     def __init__(self):
%         self.i_counter = 0

%     def on_batch_begin(self, batch, logs=None):
%         if (self.i_counter % self.reset_steps) == 0:
%             self.model.reset_states()
%             self.i_counter = 0
%         self.i_counter += 1
% \end{lstlisting}
% \begin{enumerate}
%     \item Checkpoint saving only the best one based on smalest rmse
%     \item Tensorboard tmp folder
%     \item Early stopping is applyable
%     \item Procedure of training epoch by epoch or algorithm for offline and online.
% \end{enumerate}