% Intro paragraph with init conditions of data and params
%
Since it is common for temperatures in an EV's battery pack to range from an ambient 20 degrees to the limit of 60 \textdegree{}C, all temperature ranges were used together to train each model.
The training process was conducted through all datasets for a single battery testing profile, and validated on a single cycle of unseen data of 25 \textdegree{}C (less or around 20\% of the entire set) from another two datasets.
This approach led to the accuracy being lower than that reported by other researchers, who were training individually for single temperature ranges, such as \mbox{Xiao et al.~\cite{xiao_accurate_2019}}.
The following section compares the models trained on each individual and then tests them against the entire dataset of all three profiles.
All examples were trained using charge and discharge cycles with a predetermined set of hyperparameters.
First, the evaluation of hyperparameters was carried out.
Then, all models were compared, using nine plots per method, outlining their training history, prediction, and the generalisation of other profiles per driving schedule.
All the models' deployment testing was proved to operate effectively on low-power devices regarding predictions for EV applications.
Although it is beyond the scope of this paper, in work published elsewhere, the algorithm performed adequately, indicating that, if not during training, an actual real-time prediction could work on onboard hardware.
The performance was summarised in a table representing the MAE, RMSE, and $R^2$ of each method for each profile against the entire dataset of each driving schedule.
% Metrics references
%
%%Final tests for a model performance were conducted against an entire set of two remaining profiles separately.
%%%%% > The metrics were reported using the equations outlined in the \mbox{Table~\ref{tab:metrics}}.
%%%%% > Figures were generated during each iteration of the training process from the data samples outlined in the \mbox{Subsection~\ref{subsec:b_data}}.
%%%%% > After completing the predefined amount of epochs, each metric was recorded in a comma-separated file to produce accuracy plots, allowing assessment of the efficiency of the learning process.

% Intro to the hyper-parameters calculation, the Evaluation process of layers and neurons to get the best set
%
%%%%% > The hyperparameters selection process based on evaluating the best set of layers and neurons for all research models has been determined and recorded before the primary training process.
%%%%% > \mbox{Tables~\ref{tab:param-search1} and~\ref{tab:param-search2}} based on the average of three attempts per each current profile has been produced, sorted by applicable criteria and used through the rest of the experiments.
%%%%% > The lowest mean average error and relative memory size were selected as primary research aspects.
%%%%% > It is important to note that those parameters are not universally applicable to any other kind of battery or telemetries from electric vehicles.
%%%%% > The set of hyperparameters may have been promising for the data in this research.
%%%%% > However, that evaluation would have to be repeated for other batteries or deployments with car endurance records.

%
%
%%%%% > \mbox{Table~\ref{tab:acc-results1}}, contains results of accuracy validation on six implemented models over entire drive cycles datasets.
%%%%% > \mbox{Figures between \ref{fig:Model-1res} and \ref{fig:Model-5res}} demonstrated the best-selected cases for visual demonstration and comparison of training on one profile and validation against the other two.
% \mbox{Figures \ref{fig:Model-1losses} to \ref{fig:Model-6losses}} refer to the best model over the learning process, based on minimum Mean average or Root Mean Squared errors.

\subsection{Optimisation of layers and neurons}
% Hyperparameters results, rollback and learning rate
%! 135  Models to asses. That is why only average of 3 attempts was used. Average MAE was calculated from the Average of each individual profile.
%
Between one and three layers \textit{'L'} and incremental combinations of neurons \textit{'N'} from 131 to 1572 were used to determine the optimum set of hyperparameters for all models to work with.
Any values higher or lower than both parameters did not provide worthy outputs; therefore, these were omitted.
In a case with multiple layers, the number of neurons was evenly distributed per layer and narrowed to the lowest possible integer.
For example, three layers at 1048 neurons would represent each LSTM or GRU layer, containing only~349.
%%%%%%%%%%%%%%%
Training simple LSTM models with an Adam optimiser for three current profiles only three times  resulted in a total of 135 trained models.
% because 180 models are too much to bare
These could all be summarised in 15 different hyperparameters sets for comparison.
\mbox{Table~\ref{tab:param-search1}} reports the average results of the five best models, based on the lowest average training error of all three profiles.
%% and occupied memory space in a compressed state in MegaBytes (MB).
The time in seconds highlights the duration of training over a single epoch.
Online training on a low-power device could be considered an essential factor, sacrificing some accuracy; however, this was not the case in this research.
The angle of inclination is a line fit to the average error training over time, starting from the second epoch until the end of the training.
This can be determined either by visually examining the average training curve of all attempts or through a simple line fitting, where the negative or positive reversed tangent of angle alpha represents the convergence or divergence, and constant C represents the height of the curve or average error.
%
% It reports the trend of the model performance, where alpha represents the angle and c the heights.
% In the tables below, the negative \textbf{revesersed tangent?} represents the error degradation, there as the positive a clear overfit.
As a result, a model with index 10, with three layers, and a total of 131 neurons (43 per layer) was selected as the main research hyperparameter combination for all follow-up models.
This was also the lowest memory combination, which served as another criterion in favour of this selection.
% \textbf{Angle of inclination of a line tanh-1(alpha) without adding 180 }
\ifthenelse{\boolean{thesis}}{
\begin{table}[htbp]
  \renewcommand{\arraystretch}{1.3}
  \caption{Hyperparameters' selection, sorted by average MAE.}
  \centering
  \label{tab:param-search1}
  \begin{tabular}{ l l r r r r}
      \hline\hline \\[-3mm]
      $ index $ & $L$x$N$ & $size (MB)$ & $time(s)$ & $	\angle $ inclination & \textbf{avr MAE}\\
      \hline
      10        & 3x131   & 0.17    & 2112.38   & converges            & \textbf{2.5137} \\
      11        & 3x262   & 0.63    & 2304.04   & converges            & \textbf{2.8515} \\
       6        & 2x262   & 0.85    & 1670.61   & converges            & \textbf{2.8789} \\
       5        & 2x131   & 0.22    & 1429.47   & converges            & \textbf{3.0303} \\
       7        & 2x524   & 3.33    & 1990.49   & diverges             & \textbf{3.0303} \\
       \ \ \vdots   & \ \ \vdots  & \ \ \vdots  & \ \ \vdots    & \ \ \vdots      & \ \ \vdots \\
      \hline\hline
  \end{tabular}
\end{table}
} {
\begin{table}[H]
  \caption{Hyperparameters' selection, sorted by average MAE.}
  %\centering
  \label{tab:param-search1}
  \newcolumntype{C}{>{\centering\arraybackslash}X}
  \begin{tabularx}{\textwidth}{CCCC>{\centering}m{2.5cm}C}
    \toprule
    \textbf{Index} & \textbf{N/L} & \textbf{Size (MB)} & \textbf{Time (s)} & $\pmb{\angle}$~\textbf{Inclination} & \textbf{avg. MAE}\\
    \midrule
    10  & 131/3 & 0.17  & 2112.38 & converges & 2.5137 \\
    11  & 262/3 & 0.63  & 2304.04 & converges & 2.8515 \\
    6   & 262/2 & 0.85  & 1670.61 & converges & 2.8789 \\
    5   & 131/2 & 0.22  & 1429.47 & converges & 3.0303 \\
    7   & 524/2 & 3.33  & 1990.49 & diverges  & 3.0303 \\
    \ \ \vdots & \ \ \vdots & \ \ \vdots & \ \ \vdots  & \ \ \vdots   & \ \ \vdots \\
    \bottomrule
  \end{tabularx}
\end{table}
}
% \begin{table}[htbp]
%   \renewcommand{\arraystretch}{1.3}
%   \caption{Hyper-params selection - Sorted by lightest}
%   \centering
%   \label{tab:param-search2}
%   \resizebox{\columnwidth}{!}{
%   \begin{tabular}{ l l r r r r r}
%       \hline\hline \\[-3mm]
%       $ index $ & $L$x$N$ & \textbf{size (MB)} & $time(s)$ & $	\angle $ inclination & $avr \ MAE$\\
%       \hline
%       10        & 3x131   & \textbf{0.17} & 2112.38   & converges            & 2.5137   \\
%        5        & 2x131   & \textbf{0.22} & 1429.48   & converges            & 3.0304   \\
%        0        & 1x131   & \textbf{0.30} &  846.73   & converges            & 3.7015   \\
%       11        & 3x262   & \textbf{0.64} & 2304.04   & converges            & 2.8515   \\
%        6        & 2x262   & \textbf{0.85} & 1670.61   & converges            & 2.8789   \\
%       \hline\hline
%   \end{tabular}
%   }
% \end{table}

%
% Flops
With a new set of hyperparameters of 43 neurons per three layers, each model was adapted, as well as an equivalent number of floating point operations per second (FLOPs), which used each model to provide a single output; this is reported in Table~\ref{tab:flops}.
The impact of the optimiser was not included in the calculation, nor was the complexity of the training process, due to the significant impact of testing hardware.
% and sorted by the speed in descending order. has been calculated to create a comparitive results. 
\ifthenelse{\boolean{thesis}}{
However, the table was sorted in descending order based on the relative training speed over a single sample, considering the optimiser algorithms' number of operations, as discussed in Section~\ref{sec:AN:Body}.
  \begin{table}[H]
    \caption{
      A number of floating point operations per second (FLOPs) for each model, with 3 layers and 131 neurons (43 per layer), and hyperparameters.
      The optimiser column serves as a reference.
      }
    \newcolumntype{C}{>{\centering\arraybackslash}X}
    \label{tab:flops}
    \centering
    \begin{tabular}{llcc}
        \hline \hline
        \textbf{\#} & \textbf{Type} & \textbf{FLOPs} & \textbf{Optimiser} \\
        \hline
        2  & \hyperref[fig:GRU-cell]{GRU}                  & 102127 & \hyperref[alg:ENS]{Ensemble}  \\
        &                      &  & (\hyperref[alg:nadam]{Nadam} \& \hyperref[alg:adamax]{AdaMax}) \\
        4  & \hyperref[fig:GRU-cell]{GRU}                  & 102127 & \hyperref[alg:RoAdam]{RoAdam} \\
        5  & \hyperref[fig:LSTM-cell]{LSTM}                 & 120494 & \hyperref[alg:SGDwM]{SGDw/M} \\
        1  & \hyperref[fig:LSTM-cell]{LSTM}                 & 120494 & \hyperref[alg:Adam]{Adam}  \\
        3  & \hyperref[fig:LSTM-cell]{LSTM} + \hyperref[fig:attention]{Attention}     & 207450 & \hyperref[alg:Adam]{Adam} \\
        \hline \hline
    \end{tabular}
  \end{table}
} {
However, the table was sorted in descending order based on the relative training speed over a single sample, considering the optimiser algorithms' number of operations, as discussed in Section~\ref{sec:Body}.
\begin{table}[H]
  \caption{
    A number of floating point operations per second (FLOPs) for each model, with 3 layers and 131 neurons (43 per layer), and hyperparameters.
    The optimiser column serves as a reference.
    }
    \newcolumntype{C}{>{\centering\arraybackslash}X}
    \label{tab:flops}
    \begin{tabularx}{\textwidth}{>{\centering}m{0.5cm}CCC}
      \toprule
      \textbf{\#} & \textbf{Type} & \textbf{FLOPs} & \textbf{Optimiser} \\
      \midrule
      2 & GRU        & 102,127 & Ensemble \\
      &          & & (Nadam and AdaMax) \\
      4 & GRU        & 102,127 & RoAdam \\
      5 & LSTM       & 120,494 & SGDw/M \\
      1 & LSTM       & 120,494 & Adam \\
      3 & LSTM + Attention  & 207,450 & Adam \\
      \bottomrule
    \end{tabularx}
\end{table}
}
