%
% Intro paragraph with init conditions of data and params
Since it is common for temperatures in an EVs' battery pack to range from ambient 20 degrees to the limit of 60\textdegree{}C, all temperature ranges were used together to train each model.
The training process was conducted through all datasets for a single battery testing profile, and validated on a single cycle of unseen data of 25\textdegree{}C (less or around 20\% of the entire set) from another two. 
This approach led to the accuracy being lower than other researchers reported by training individually for single temp ranges such as Xiao \textit{et al.}~\cite{xiao_accurate_2019}.
The following section compares the models trained on each individual and then tests against the entire dataset of all three profiles.
All examples were trained using charge and discharge cycles with a predetermined set of hyperparameters. %% to make an objective comparison.
First, the evaluation of hyperparameters was carried out.
Then all models were compared by nine plots per method, outlining training history, prediction on itself, and generalisation of other profiles per each driving schedule.
In the end, the performance was summarised in a table, representing MAE, RMSE and $R^2$ of each method per profile against the entire dataset of each driving schedule.
% Metrics references
%
%%Final tests for a model performance were conducted against an entire set of two remaining profiles separately.
%%%%% > The metrics were reported using the equations outlined in the \mbox{Table~\ref{tab:metrics}}.
%%%%% > Figures were generated during each iteration of the training process from the data samples outlined in the \mbox{Subsection~\ref{subsec:b_data}}.
%%%%% > After completing the predefined amount of epochs, each metric was recorded in a comma-separated file to produce accuracy plots, allowing assessment of the efficiency of the learning process.

% Intro to the hyper-parameters calculation, the Evaluation process of layers and neurons to get the best set
%
%%%%% > The hyperparameters selection process based on evaluating the best set of layers and neurons for all research models has been determined and recorded before the primary training process.
%%%%% > \mbox{Tables~\ref{tab:param-search1} and~\ref{tab:param-search2}} based on the average of three attempts per each current profile has been produced, sorted by applicable criteria and used through the rest of the experiments.
%%%%% > The lowest mean average error and relative memory size were selected as primary research aspects.
%%%%% > It is important to note that those parameters are not universally applicable to any other kind of battery or telemetries from electric vehicles.
%%%%% > The set of hyperparameters may have been promising for the data in this research.
%%%%% > However, that evaluation would have to be repeated for other batteries or deployments with car endurance records.

%
%
%%%%% > \mbox{Table~\ref{tab:acc-results1}}, contains results of accuracy validation on six implemented models over entire drive cycles datasets.
%%%%% > \mbox{Figures between \ref{fig:Model-1res} and \ref{fig:Model-5res}} demonstrated the best-selected cases for visual demonstration and comparison of training on one profile and validation against the other two.
% \mbox{Figures \ref{fig:Model-1losses} to \ref{fig:Model-6losses}} refer to the best model over the learning process, based on minimum Mean average or Root Mean Squared errors.

\subsection{Optimisation of layers and neurons}
%
%! Hyperparameters results, rollback and learning rate
%! 135  Models to asses. That is why only average of 3 attempts was used. Average MAE was calculated from the Average of each individual profile.
The ranges between 1-3 layers \textit{'L'} and incremental combinations of neurons \textit{'N'} from 131-1572 were used to determine the most optimum set of hyperparameters for all models to work with.
Any values higher or lower than both parameters did not provide worthy outputs and therefore have been omitted.
In a case with multiple layers, the number of neurons gets evenly distributed per layer, narrowed to the lowest integer.
For example, three layers at 1048 neurons would represent each LSTM or GRU layer containing only 349.
%%%%%%%%%%%%%%%
Training simple LSTM models with Adam optimiser for three current profiles three times only resulted in a total of 135 trained models.
% because 180 models are too much to bare
They all can be summarised in 15 different sets of hyperparameters to compare.
\mbox{Table~\ref{tab:param-search1}} reports the average results of the five best models based on the lowest average training error of all three profiles.
%% and occupied memory space in a compressed state in MegaBytes (MB).
The time in seconds highlights the duration of training over a single epoch.
Online training on a low-power device could be considered an essential factor, sacrificing some accuracy, but not in this research.
The angle of inclination is a line fit to the average error training over time, starting from the second epoch until the end of the training.
It can be determined either by visually examining the average training curve of all attempts or through a simple line fitting, where negative or positive reversed tangent of angle alpha will represent converges or divergence and constant C the height of the curve or average error.
%
% It reports the trend of the model performance, where alpha represents the angle and c the heights.
% In the tables below, the negative \textbf{revesersed tangent?} represents the error degradation, there as the positive a clear overfit.
As a result, a model with index 10 with three layers and a total of 131 neurons (43 per layer) has been selected as the main researched hyperparameters combination for all follow-up models.
This was also the lowest memory combination, which served as another criterion in favour of this selection.
% \textbf{Angle of inclination of a line tanh-1(alpha) without adding 180 }
\begin{table}[htbp]
  \renewcommand{\arraystretch}{1.3}
  \caption{Hyper-params selection - Sorted by Average MAE}
  \centering
  \label{tab:param-search1}
  \begin{tabular}{ l l r r r r}
      \hline\hline \\[-3mm]
      $ index $ & $L$x$N$ & $size (MB)$ & $time(s)$ & $	\angle $ inclination & \textbf{avr MAE}\\
      \hline
      10        & 3x131   & 0.17    & 2112.38   & converges            & \textbf{2.5137} \\
      11        & 3x262   & 0.63    & 2304.04   & converges            & \textbf{2.8515} \\
       6        & 2x262   & 0.85    & 1670.61   & converges            & \textbf{2.8789} \\
       5        & 2x131   & 0.22    & 1429.47   & converges            & \textbf{3.0303} \\
       7        & 2x524   & 3.33    & 1990.49   & diverges             & \textbf{3.0303} \\
      \hline\hline
  \end{tabular}
\end{table}
% \begin{table}[htbp]
%   \renewcommand{\arraystretch}{1.3}
%   \caption{Hyper-params selection - Sorted by lightest}
%   \centering
%   \label{tab:param-search2}
%   \resizebox{\columnwidth}{!}{
%   \begin{tabular}{ l l r r r r r}
%       \hline\hline \\[-3mm]
%       $ index $ & $L$x$N$ & \textbf{size (MB)} & $time(s)$ & $	\angle $ inclination & $avr \ MAE$\\
%       \hline
%       10        & 3x131   & \textbf{0.17} & 2112.38   & converges            & 2.5137   \\
%        5        & 2x131   & \textbf{0.22} & 1429.48   & converges            & 3.0304   \\
%        0        & 1x131   & \textbf{0.30} &  846.73   & converges            & 3.7015   \\
%       11        & 3x262   & \textbf{0.64} & 2304.04   & converges            & 2.8515   \\
%        6        & 2x262   & \textbf{0.85} & 1670.61   & converges            & 2.8789   \\
%       \hline\hline
%   \end{tabular}
%   }
% \end{table}

