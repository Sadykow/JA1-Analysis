Since it is common for temperatures in the accumulator inside an EV to spike from ambient 20 degrees to the limit of 60, all temperature ranges were used together to train each model.
The training process was conducted through all datasets for a single battery testing profile and validated on a single cycle of unseen data of 25\textdegree{}C (less or around 20\% of the entire set).
This approach led to the accuracy being lower than other researchers reported by training individually for temp ranges like with Xiao et al.~\cite{xiao_accurate_2019}.
The following section compares the models trained on each individual and then tests against the entire dataset of all three profiles.
All examples were trained using charge and discharge cycles and an initially predetermined set of hyperparameters to make an objective comparison.

%
%
% Final tests for a model performance were conducted against an entire set of two remaining profiles separately.
The metrics were reported using the equations outlined in the \mbox{Table~\ref{tab:metrics}}.
Figures were generated during each iteration of the training process from the data samples outlined in the \mbox{Subsection~\ref{subsec:b_data}}.
After completing the predefined amount of epochs, each metric was recorded in a comma-separated file to produce accuracy plots, allowing assessment of the efficiency of the learning process.

% The Evaluation process of layers and neurons to get the best set
%
The hyperparameters selection process based on evaluating the best set of layers and neurons for all research models has been determined and recorded before the primary training process.
\mbox{Tables~\ref{tab:param-search1} and~\ref{tab:param-search2}} based on the average of three attempts per each current profile has been produced, sorted by applicable criteria and used through the rest of the experiments.
The lowest mean average error and relative memory size were selected as primary research aspects.
It is important to note that those parameters are not universally applicable to any other kind of battery or telemetries from electric vehicles.
The set of hyperparameters may have been a good one for applicable data in this research.
However, that evaluation would have to be repeated for other batteries or deployments with car endurance records.

%
%
\mbox{Two tables, \ref{tab:acc-results1} and \ref{tab:acc-results2}}, contains results of accuracy validation on six implemented models over entire drive cycles datasets.
\mbox{Figures between \ref{fig:Model-1res} and \ref{fig:Model-5res}} demonstrated the best-selected cases for visual demonstration and comparison of training on one profile and validation against the other two.
% \mbox{Figures \ref{fig:Model-1losses} to \ref{fig:Model-6losses}} refer to the best model over the learning process, based on minimum Mean average or Root Mean Squared errors.

%
%
\subsection{Hyperparameters evaluation}
The ranges between 1 to 3 layers and incremental combinations of neurons from 131 to 1572 were used to determine the most optimum set of hyperparameters for all models to work with.
Any values higher or lower of both parameters did not provide worthy outputs, therefore, has been amended from the research.
In a case with multiple layers, the number of neurons gets evenly distributed per layer, narrowed to the lowest integer.
For example, three layers at 1048 neurons would represent each LSTM or GRU layer containing only 349.
%%%%%%%%%%%%%%%
Training simple LSTM models with Adam optimiser for three current profiles three times resulted in a total of 135 trained models.
They all can be summarised in 15 different sets of hyperparameters to compare.
\mbox{Tables~\ref{tab:param-search1} and~\ref{tab:param-search2}} report the average results of the five best models based on the lowest average training error and occupied memory space in a compressed state in MegaBytes (MB).
The time in seconds highlights the duration of training over a single epoch.
For online training on a low-power device, it could be considered as an essential factor, sacrificing some amount of accuracy, but not in this research.
The angle of inclination is a line fit to the average error training over time, starting from the second epoch until the end of the training.
It can be determined either by visual examination of the average training curve of all attempts or through a simple line fitting, where negative or positive reversed tangent of angle alpha will represent converges or divergence and constant C the height of the curve or average error.
%
% It reports the trend of the model performance, where alpha represents the angle and c the hights.
% In the tables below, the negative \textbf{revesersed tangent?} represents the error degradation, there as the positive a clear overfit.
As a result, a model with index 10 with three layers and a total of 131 neurons (43 per layer) has been selected as the main researched hyperparameter for all follow-up models.
% \textbf{Angle of inclination of a line tanh-1(alpha) without adding 180 }
\begin{table}[htbp]
  \renewcommand{\arraystretch}{1.3}
  \caption{Hyper-params selection - Sorted by Average MAE}
  \centering
  \label{tab:param-search1}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{ l l r r r r}
      \hline\hline \\[-3mm]
      $ index $ & $L$x$N$ & $size (MB)$ & $time(s)$ & $	\angle $ inclination & \textbf{avr MAE}\\
      \hline
      10        & 3x131   & 0.17    & 2112.38   & converges            & \textbf{2.5137} \\
      11        & 3x262   & 0.63    & 2304.04   & converges            & \textbf{2.8515} \\
       6        & 2x262   & 0.85    & 1670.61   & converges            & \textbf{2.8789} \\
       5        & 2x131   & 0.22    & 1429.47   & converges            & \textbf{3.0303} \\
       7        & 2x524   & 3.33    & 1990.49   & diverges             & \textbf{3.0303} \\
      \hline\hline
  \end{tabular}
  }
\end{table}
\begin{table}[htbp]
  \renewcommand{\arraystretch}{1.3}
  \caption{Hyper-params selection - Sorted by lightest}
  \centering
  \label{tab:param-search2}
  \resizebox{\columnwidth}{!}{
  \begin{tabular}{ l l r r r r r}
      \hline\hline \\[-3mm]
      $ index $ & $L$x$N$ & \textbf{size (MB)} & $time(s)$ & $	\angle $ inclination & $avr \ MAE$\\
      \hline
      10        & 3x131   & \textbf{0.17} & 2112.38   & converges            & 2.5137   \\
       5        & 2x131   & \textbf{0.22} & 1429.48   & converges            & 3.0304   \\
       0        & 1x131   & \textbf{0.30} &  846.73   & converges            & 3.7015   \\
      11        & 3x262   & \textbf{0.64} & 2304.04   & converges            & 2.8515   \\
       6        & 2x262   & \textbf{0.85} & 1670.61   & converges            & 2.8789   \\
      \hline\hline
  \end{tabular}
  }
\end{table}

%
%
\subsection{Models results overview}
Implementation of several variations of time-series models allowed analysis of multiple paths of evolution of machine learning techniques in State of Charge estimation over the past three years.
A review of the resulting pros and cons, accuracies and complexity helps make a reasonable justification for further improvements.
Based on the summary of accuracies across entire training sets of all three profiles Table~\ref{tab:acc-results1}, the Highway way driving achieved the highest error on validation and testing against either profile for all models.
Further observation will focus primarily on DST and FUDS-based models and their efficiency at generalising each-others and US06 profiles.
The set of figures~\ref{fig:Model-1res}-\ref{fig:Model-4res} and early mentioned Table will act as two primary sources of the results for further overview and discussion.

%
%
Model \#1 has been based on the most simple and oldest research, made by Chemali \textit{et al.}~\cite{Chemali2017} in 2017.
Unlike during this research, back then, it utilised the simplest model structure with a single layer and no complicated cell modifications or optimiser enhancements.
% We used all temperature ranges, whereas he only 1-3. His method had 3.31 at out conditions; we lowered it to 2.51
While they utilised from 1 to 3 temperature ranges, the modification of the hyperparameter increased the model's efficiency by 25\% at five different ambients, although not achieving equally good results due to different methodologies and research goals.
% 2.77-ours, there his 0.77. He did Either C-D on one, or D on 3
However, with 2-3 times increased complexity of the input data, due to training on charge and discharge across five temperature ranges, the miss-accuracy only trippled, remaining below 5\% of an error.
Based on Table~\ref{tab:acc-results1}, it has the most stable convergence across three different profiles compared to the other attempts.
%!---It can be justified by the longest training time to reach the lowest error.
This model has not shown good accuracies on US06 and FUDS datasets, unlike with DST, due to the general simplicity of current behaviour.
%!---Contrary to DST, the US06 made the best result in capturing both features.
%!---By analysing Table~\ref{tab:acc-results1}, US06 and FUDS results for Model \#1 can capture each other behaviour, but not the DST.


%
%
\textbf{TOBE Updated with values upon readiness.}
Model \#2 was an attempt to move from an old LSTM to a recently developed GPU type of cell.
Both Adam and Nadam optimisers by themselves ran for half less epoch than LSTM, but achieved similar accuracies.
Embedding an additional optimiser with the same learning rate scheduling for fine-tuning purposes doubled and tripled the training time.
% However, it utilised two different optimisers for the pre and fine-tuning phases.
% The transition happened after a third of the maximum allowed iterations, which in some cases led to high accuracy spikes in the training process, as seen in Figure~\ref{fig:Model-2losses}.
% The pre-tuning phase might have decreased the time to achieve the optimal results, but considering the number of input samples used - the difference is not noticeable between the LSTM and GPU models.
Lowering the learning rate and switching the optimiser led to a much more stable learning curve but did not bring any improved prediction results.
% Despite that, training went smoother, except for minor anomalies on the FUDS training as per Model~\ref{fig:Model-2res}, the overall behaviour capture across profiles did not improve.
%\textcolor{red}{Matt: Do I even need to share my thoughts and refer to Table with potential why results worsened? I don't have an answer myself yet}

%
%
%! Model 3 repeats the same procedures as Model 1, except for an additional layer before the output.
Similar to Model \#1, Mamo~\textit{et al.}~\cite{mamo_long_2020} utilised a single temperature rage, but with single or two profiles of DST, US06 or FUDS.
Their use of only the discharge cycle does not allow direct comparison between results or plots.
%Adding the attention layer as per Model \# 3 did not make an overall improvement system improvement, but it produced much smoother outputs.
However, the prediction has a lesser variant and smoother behaviour referring to a state of charge. 
The Absolute error fill showed none or fewer spike predictions on validation data.
Results on the DST model did not show noticeable improvements as opposed to the Model \#1, but still shows the lowest testing error against two other profiles.
%! However, more complex profiles like US06 FUDS models and overall capture are reasonable better.
However, the general accuracy on more trendies profiles demonstrated a noticeable average improvement.
Validation illustrates the closer capture of the SoC above 30\% of the charge at FUDS.
The behaviour of the DST-based model is similar to Model \#1, with the same flaws in the charge and discharge curves.
%!---- Model \#3 was able to achieve the lowest error twice faster but also went to overfit without learning step adjustment, as per Figure~\ref{fig:Model-3losses}.
%! ----It could make a reasonably good capture of all three profiles with US06 profiles.
Then as the FUDS model became much smoother in training with the attention layer, but still has difficulty in capturing above 40\% charge in the testing scenarios compared to the DST version, subfigure~\ref{subfig:Model-3res-testing}. 
Unlike a simpler model, the testing clearly shows that the training line converges over time, which indicates an improvement toward generalising the more complex behaviours.
% improved on the FUDS training and better fir on Figure 9.i.
% Instable, variet, noisy

%
%
% Unlike other methods, Model \#4 utilised a stateful technique for the data input pipeline.
% Despite that, it has a promising approach but only in specific scenarios.
% For example, the training over DST has been conducted with both charge and discharge together, resetting the model at the end of the cycle, as per subfigure~\ref{subfig:Model-4res-DSTvsDST}.
% Having a charge in the system added complexity to the capture, but the error had a stable degradation over time as per Figure~\ref{fig:Model-4losses}.
% However, it had to be cut off early since the training threshold had not reached the set limit.
% It was attempted to be fixed on US06 and FUDS-based models, applying only to the discharge process.
% However, it did not bring much improvement and only made charge prediction more accurate than DST.
% For the general behaviour capture, stateful models are not suited by themself only.
% However, training two models separately for both charge and discharge or use it additional techniques on short prediction, for example, during acceleration events - the stateful model has usable applications.

%
%
Similarly to the second method, Model \#4 applied a different optimiser.
However, additional complexity to other processing introduced better overall accuracy Table~\ref{tab:acc-results1}, as reported by Javid~\textit{et al.}~\cite{javid_adaptive_2020}.
All three models experienced better training convergence in a shorter time, as per Figure~\ref{fig:Model-4res}.
According to all three profiles, improved optimisers lead to better capture on the validation set but further on the testing.
Figures show the improved capture at the isolated case but worse general capture as opposed to other models.
On Average, the GRU models achieve an optimum three times faster than LSTM.
So, the prolonged utilisation of the model results from an optimiser, not a type of a cells
However, the difficulty in capturing the below 30\% of a charge is still preserved.
Unlike other models, this one had a good recovery ability with a learning rate lowering.
Generally, it took 2-5 attempts to return to the initially targeted minimum.
%! Unlike Model 3, Model 4 failed to provide better cherectrisation to the area below 30 degrees, producing fluctiative reading.
% Model 5: 1,3 model applied modification to the structure directly and fourth one indirectly. (2nd one did not, read again).
% There as (2?) and 5 improve optimisation steps, leading to better accuracy in all testing.
%!---- RoAdam allowed faster convergences to the optimal accuracy and generaly achioeved better results by Table 2.

%
%
\textbf{Model 5 Placeholder. The importance of the simplest optimiser does not mean better or more engineering efficiency if it takes 100+ epochs to achieve results. In general, it has the least steep learning curve in comparison to others} \\
Model 5 implemented the most straightforward case to verify the efficiency of standardly used Adam optimisers.
Also, it acts as an indication to explore the cell types' limitations and where they are failing***.
Model \#5 is an indicator of the areas ML models have difficulties capturing.
Those are the charge above 30\%, some degree between 90-80\% of discharge and below 50\%.
Those areas distinguish the most different temperatures, causing difficulties in capturing all of them.
% Model \#6 applied an additional LSTM layer with separated neurons to test better capturing.
% However, the result did not bring better error or faster convergence.
% In some cases, like wi\mbox{Two tables, \ref{tab:acc-results1} and \ref{tab:acc-results2}}, contains results of accuracy validation on six implemented models over entire drive cycles datasets.
% \mbox{Figures between \ref{fig:Model-1res} and \ref{fig:Model-6res}} demonstrated the best-selected cases for visual demonstration and comparison of training on one profile and validation against the other two.
% \mbox{Figures \ref{fig:Model-1losses} to \ref{fig:Model-6losses}} refer to the best model over the learning process, based on minimum Mean average or Root Mean Squared errors.

%
%
\subsection{Observations and discussions}
%
%
The initial methodology tested the impact of the neurons and layers on the model's accuracy to fit battery data.
It tested an average of multiple sets and recorded several metrics to determine the best combination based on accuracy and size.
Although the ultimate combination has not been achieved, it concluded that the higher numbers do not lead to better results.
Doubling the number of neurons per layer may triple an occupied space without any accuracy or latency advantage.

%
%
After analysing all five models, several comparisons can be derived from them.
Despite multiple tests over two cell types, there is no apparent advantage in using LSTM or GRU layers.
Two versions of Model 1 with LSTM and GRU were tested to determine the impact of cell type.
Both models tended to achieve the same accuracy but at different times.
GRU generally achieves the optimum after 7-10 epochs, without the potential to recover, unlike LSTM. % after 50 learning rate degradations.
The size of GRU was 30kB lower than LSTM, which is explained by a lower number of gates per neuron.
%Models \#1 and \#2 give an illusion of an advantage to one model over another.
%However, the accuracy plots in \mbox{Figures~\ref{fig:Model-1losses} and ~\ref{fig:Model-2losses}} indicate how error degrades with time for both models.

%

%
%
Model \#5 \#1 and \#4 have tested the impact of the increased complexity of gradient-based optimiser.
The first one demonstrated the speed at which a model achieves the optimum accuracy with minimal implementation.
Since the Adam optimiser is inbuilt of SGD, it leads to an increased speed in the fitting process.
The RoAdam introduced additional variables and equations to impact the learning process directly via loss value.
It prolonged the training process without hitting an early overfit, improving the capture on the more complex profiles, like FUDS, giving no significant impact on DST. 

%
%
Model \#2 attempts to achieve the same thing as Model \#4 but focuses on already well-tested optimisers rather than modifying one of them. 
The ensemble used two algorithms to speed up the long process of locating the local minimum and the second to tune up closer to it with an adjusted learning step.
Model \#2 had better-avoided overfitting since it used two optimisers for quick adjustment and tuning.
However, the original author's goal was to achieve the best minimum and tune the actual value as closely as possible.
Unfortunately, the methodology attempted to exploit multiple minimal independently and take the average, breaking the original intention of producing a single good model, which will be based on a single profile only for training and testing.
% Model \#2 and \#4 approach the convergence with optimisers modification.
%
The pre-tune phase of Model \#2 and results of Model \#1 indicates similarity in the error degradation, comparing Nadam and Adam optimisers.
Their efficiency has been considered a trivial process to research since the difference is not as notable as between Adam and RoAdam.

%The advantage of modifying optimisers is better observed on the accuracy plots, \mbox{Figure~\ref{fig:Model-2losses} and ~\ref{fig:Model-5losses}}.
%However, the modified Adam version directly impacting the loss ratio improved the result drastically.
% Similar to Model \#4, which had a small learning rate, but modified parameter update with direct involvement of the loss values.
% An early termination over \#5 and \#6 is a result of overfitting or apparent stability in the accuracy.
%With the number of samples that the training process went through and based on the RMSE plot, there was little need for repeated training over the same data, as proven in the first several models.
%According to all thee profiles, an improved by a direct influence of loss function allows certain degree of adjustment, which is worth to consider.

%
%
Models \#3 with the Attention layer captured complex profiles with higher accuracy like FUDS as opposed to DST, which is visible in comparison with Model \#1 plots.
Since the testing curve on the FUDS profile showed clear converges, unlike Model \#1, the Attention layer, along with an improved optimiser, could have led to even further improvement, as per Mamo~\textit{et al.}~\cite{mamo_long_2020} work.
Mamo combined the Attention layer with a statistical optimiser to work out multiple profile training and testing on a single.
However, the use of Differential Evolution was out of the scope of the research, focusing only on gradient optimisers.
Besides, the hyperparameters were fixed for all Models and predetermined beforehand, unlike in the original case.
%* Size: 176328 bytes - Model 1
%* Size: 187900 bytes - Model 3
%* LEad to additional 11572 bytes in size after default compression.
While changes in the neurons and layers lead to 50-500 KBs size increase, an Attention layer added only extra 11.5KBs of storage memory used after compression.
% The attention layer may not significantly boost the training or accuracy, but it gives a good foundation for further improvements and modifications.
Although it had a promising start by the training accuracy, the technique of adding more modified functions as a means to make the model better rather than just adding more layers or neurons to the model is a promising direction to explore.
% There as Model \#3 introduced additional custom computation without extra memory cells, leading to a smoother output.


%
%! [Refer to the bottom dicussion of stateful model]
% Model \#4 was the only one, which used a stateful model for training and testing.
% \textbf{Since every time-series model has a stateful internally, the difference is in the point of the model's reset.}
% It is more convenient for the State of Charge scenario to work with a short burst of data rather than attempt to preserve very long dependencies in a single run and be dependent to initial conditions.
% %
% %
% The best performance with Stateful models can be achieved through using a separate training process for charge and discharge. 
% \mbox{Subfigure~\ref{subfig:Model-4res-DSTvsDST}} demonstrates training over DST, which sufferers from high error in both charge and discharge process.
% The other two training were performed with discharge sets only, \mbox{Subfigures~\ref{subfig:Model-4res-UStr}, \ref{subfig:Model-4res-FUDStr}}.
% Stateful models can not be validated using traditional means of accurate measurement.
% \mbox{Table~\ref{tab:acc-results2}} for Model \#4 takes results from a single cycle only, same as~\ref{fig:Model-4res}, since there is no straightforward way to validate across all cycles (marked with $*$ symbol).

%
%
% Acubs as a comparison to model 1.
% However, the US06 acted as a better model in ..., due to AAA!!!!
%
% Separating multiple layers to the same amount of units did not lead to improvements.
% It was the fastest what achieved the lowest training accuracy, but without affecting learning rate, model could not achieve capturing the other trens.
%
%
%
% This all shown that model not able to capture middle area.
% However, combining all 3 techniques into single model may lead to accurate results.
%
%
%
% Model 1,5,6 may be matter of randomness, more that just technique eficiency.
% DST generally faster singe profile itsekf is very easy to handle.
% + Model 4 had no means to perform test on entire set of both profiles.
%%%%%%%%%%%%%%%%%%%%
%% PUT both writing and the results from. After you do thatm compare the results as discussion. Expplicitly interpreter that.
%
%
% \subsection{******}
%
% 
%
%
Voltage evolution during charge and discharge cycles is a primary source of characterising the behaviour of the State of Charge.
Therefore, distinguishing the flat region of the voltage of 3.3V (around 70-30\% of discharge) creates difficulties for an ML model in the least apparent current consumption cases, such as FUDS, due to Voltage Drops via Internal Resistance of a battery.
The better models' capability to identify their trained profile, the worse they become in characterising the others.
% \textbf{Capturing the middle area of charge, where flat-region of the voltage remains stable most of the time, is difficult, and DST may not be the best for isolated case.}
%
\textcolor{red}{However, the comparison Figure~\ref{fig:Model-1res} against tabled results creates a false impression that FUDS does a better training to capture other profiles in an isolated case, the table contradicts that in the overall FUDSs doubles the error on other profiles, as opposed to DSTs.}
\textcolor{blue}{However, by analysing Figure~\ref{fig:Model-1res}, the DST had the best results in capturing the general behaviour of all three profiles due to its' simplicity and repentance in the current consumption, roughly 365 seconds per each subcycle, as opposed to 600 and 1400 for US06 and FUDS.}
Subfigure~\ref{subfig:Model-1res-DSTvsFUDS} illustrates the area plot with the highest peaks compared to other plots.
\textcolor{red}{It achieved the best capture of other profiles but not the best at validation.
Meaning that without reaching the lowest optimum error due to a simpler optimiser, it did not lose the capture of either profile for all 30 training cases.}
Generally, DST is bad for capturing, as shown on all plots across SoC prediction on validation.
%Although there was a chance that the Dynamic stress test may act as a middle ground between Urban and highway driving.
Although, that offset from it, which can be observed across all models, makes a better characterisation of the SoC curve since the top and bottom percentage of the charge remains the same yet is capable of characterising all temperature ranges and profiles within 95-97\% accuracy.
Such results make a good start for a shipped ML-based SoC model, which will later receive online training to fit the usability scenario.

%
%
Since the SoC estimation is not a pure number-based behaviour but also a matter of physical and electrical properties, manual adjustment weights, losses or data itself will not bring valuable results.
Further research and adjustments must be made using a similar principle to improve the training procedure with augmented models. 
Such as the rollback feature, which has proven to be adequate to keep researched models within close range of epoch to calculate average, as well as reduce the time to training all 135??.
\textbf{The techniques of determining optimal hyperparameters, efficient data selection and management, along with a promising direction toward further improvement, will assist in the development of a universal SoC estimation model.}

Overall, there is an obvious advantage in training a model over a single profile and testing against similar scenarios, for example, creating a model that fits a single drive's driving behaviour over specific driving scenarios.
However, models placed under other conditions without a post-training process with new data will suffer from inaccuracies in the estimation.
It is doubtful that one of those models will be efficient in a different condition or with another battery, despite its' age.
Comparison of the validation and testing data act as determining proof.
%
Continued improvement on gradient optimiser appears to be a dead end.
A RoAdam optimiser or combination of two makes the **best** results so far.
However, the research did not provide any means or possibilities to build optimisations further.
Therefore, modifying a cell type, models' structure, and training procedures appears to be a more promising direction.
