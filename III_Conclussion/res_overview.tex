
% 150  Models to work with
% Implementation of several variations of time-series models allowed analysis of multiple paths of evolution of machine learning techniques in State of Charge estimation over the past three years.
% A review of the resulting pros and cons, accuracies and complexity helps make a reasonable justification for further improvements.
Results are presented for all tests on five models herein.
All results presented in figures~\ref{fig:Model-1res}-\ref{fig:Model-5res}.
The total errors for each model are summarised in Table~\ref{tab:acc-results1}.
% Based on the summary of accuracies, Table~\ref{tab:acc-results1}, the highway way driving (US06) achieved the highest error on validation and testing against either profile for all models.
In total, 150 models have been produced, recorded and evaluated to meet all methodology requirements.
% Further observation will focus primarily on DST and FUDS-based models and their efficiency at generalising each-others and US06 profiles.
% The set of figures~\ref{fig:Model-1res}-\ref{fig:Model-4res} and early mentioned Table will act as two primary sources of the results for further overview and discussion.

%?? Just observe simple trends, don't discuss your unseccesful attempts or unrelated shift. What are the results?
%? #1-4 pretty good at testing on other set (1.5-3)
Based on observation of both figures and table with overall results, it can be concluded that Models 1 to 4 achieved great results, considering the complexity of the given task in the amount of input data, with training accuracies between 1.58\% to 3.37\%.
All fourth models showed steady or converging training and testing curves on their dataset at the history plots on subfigures a, d and g for DST, US06 and FUDS respectively.
%? #5 was bad at testing on training set (3-5) simply not reach to given time
Model 5 showed the worst results being the most simplest and less common nowadays due to its simplicity in implementation and therefore lack of efficiency.
Besides the average errors being in 3.08\% to 4.78\%, the history curve showed clear divergence on the testing curve. 
Because of the very small training fitting rate, it could not manage to achieve relatively same accuracy on training dataset in comparison to other models.
%! WHere do I show that the whole point is having 5 is to convince that if model does not go to 100 it does not mean that it implemented badly, it simply achieved accuracy faster, where as this one may take forever to even get close to results.
%? In all cases DST trained model performed best for testing on other drive cycles
Overall, in all cases which managed to reach the optimum point, the FUDS dataset showed the best results in capturing the complex behaviour, with the best being on Model 4, utilising a Robust Adam dataset,
whereas the DST-based model showed great results in capturing the behaviour other datasets.
%*DST  1: 0.09-0.51 -- 2: 1.31-1.68 -- 3: 0.05-0.87 -- 4: 0.93-1.22
%*FUDS 1: 3.08-2.07 -- 2: 4.02-1.31 -- 3: 3.51-1.79 -- 4: 4.16-2.45
The error variance between training and testing results in the DST case in general within 0.05-1.31\% for US06 and 0.51-1.68\% for FUDS, as opposed to FUDS being 3.08-4.16\% for DST and 1.31-2.45\% for US06.

%? #1 DST trained is best (Average of 2.86, 2.77,3.28)
%*Avg: 1 - 2.97%, 2 - 4.07%, 3 - 3.16%, 4 - 3.60%, 5 - 4.62
DST-trained Model 1 showed the best testing results, with the average accuracy of training and testing at 2.97\%.
%? #3 DST trained second best
Model 3 showed the second-best results for the same profile at 3.16\% with only a 0.19\% difference, which can be considered meaningless given the number of applied attempts.
%? #4 3rd best with #2
Model 4 comes as the third-best, sharing similarities in implementation specifics with Model 2, with values being 3.60\% and 4.07\% respectively.
%*Plots-Avg: 1 - 3.245%, 2 - 3.995%, 3 - 3.37%, 4 - 3.45%, 5 - 4.44
The same trend can be observed by following training and testing plots only, where Models 1, 3, 4 and 2 end up with 3.245, 3.7, 3.45 and 3.995 average percentage errors.
%* **verably, accurate when tested on the same drive cycles as training (eg DST on DST), with MSE in the range 1.58-3.45%. #5 less accurate that any (3.08-4.78 )





%
%
% Unlike other methods, Model \#4* utilised a stateful technique for the data input pipeline.
% Despite that, it has a promising approach but only in specific scenarios.
% For example, the training over DST has been conducted with both charge and discharge together, resetting the model at the end of the cycle, as per subfigure~\ref{subfig:Model-4res-DSTvsDST}.
% Having a charge in the system added complexity to the capture, but the error had a stable degradation over time as per Figure~\ref{fig:Model-4losses}.
% However, it had to be cut off early since the training threshold had not reached the set limit.
% It was attempted to be fixed on US06 and FUDS-based models, applying only to the discharge process.
% However, it did not bring much improvement and only made charge prediction more accurate than DST.
% For the general behaviour capture, stateful models are not suited by themself only.
% However, training two models separately for both charge and discharge or use it additional techniques on short prediction, for example, during acceleration events - the stateful model has usable applications.

%
%
% Model \#6 applied an additional LSTM layer with separated neurons to test better capturing.
% However, the result did not bring better error or faster convergence.
% In some cases, like wi\mbox{Two tables, \ref{tab:acc-results1} and \ref{tab:acc-results2}}, contains results of accuracy validation on six implemented models over entire drive cycles datasets.
% \mbox{Figures between \ref{fig:Model-1res} and \ref{fig:Model-6res}} demonstrated the best-selected cases for visual demonstration and comparison of training on one profile and validation against the other two.
% \mbox{Figures \ref{fig:Model-1losses} to \ref{fig:Model-6losses}} refer to the best model over the learning process, based on minimum Mean average or Root Mean Squared errors.
